<?xml version="1.0" encoding="utf-8"?>
  <feed xmlns="http://www.w3.org/2005/Atom" xml:base="https://conwy.co">
  <title>conwy</title>
  <subtitle>Blog articles by Jonathan Conway</subtitle>
  <link href="https://conwy.co/feed.xml" rel="self"/>
  <link href="https://conwy.co/"/>
  <updated>2024-09-13T00:00:00.000Z</updated>
  <id>https://conwy.co</id>
  <author>
    <name>Jonathan Conway</name>
    <email>jon@conwy.co</email>
  </author>
  
  <entry>
    <title>AI user interface patterns</title>
    <link href="https://conwy.co/articles/ai-patterns" />
    <updated>2024-09-13T00:00:00.000Z</updated>
    <id>ai-patterns</id>
    <content xml:lang="en" type="html">As AI capabilities become increasingly available to everyday development teams, it might be useful to explore way to surface these capabilities to end-users.

This article describes some user interface patterns for surfacing AI.

Using ***&quot;treat it like a person&quot;*** as a core principle, I provide patterns,
examples and ways in which they mimick humans.

## Principles

As the definition of &quot;AI&quot; is still somewhat in flux, I want to assume a definition for the purposes of this article:

***The capability of software/machines to do things that are normally thought of a human, in a rational way.***

The fact AI mimicks human thinking/behaviour suggests that we user interfaces should expose AI capabilities in a human way.

As Ethan Mollick puts it in the book _Co-Intelligence_ (bold mine):

&gt; AI doesn‚Äôt act like software, but it does act like a human being. I‚Äôm not suggesting that AI systems are sentient like humans,
&gt; or that they will ever be. Instead, I‚Äôm proposing a pragmatic approach: ***treat AI as if it were human*** because, in many ways,
&gt; it behaves like one. This mindset, which echoes my ‚Äútreat it like a person‚Äù principle of AI, can significantly improve your
&gt; understanding of how and when to use AI in a practical, if not technical, sense.

So let&#39;s look at the patterns...

## Pattern: Inline suggestion

While performing a task, relevant ideas are displayed nearby in text or even imagery.

The user might be invited to provide input, such as selecting between variants or filling in a blank. (See [Fill-in-the-blanks pattern](#pattern-fill-in-the-blanks))

![Example: inline suggestion for a product owner entering a task](/images/articles/ai-patterns/ai-ui-patterns-inline-suggestion.svg)

**Example:** As a product owner enters a story into a task tracking system, the AI suggests edge cases they didn&#39;t yet consider.

**Mimicks:** üßç Colleague, mentor, friend, etc. sitting nearby verbally offering a suggestion and/or physically pointing to a part of the screen.

## Pattern: Single-agent chat

A specialised chat bot is available for back-and-forth discussion.

![Example: specialised single-agent chat bot on an e-commerce website](/images/articles/ai-patterns/ai-ui-patterns-single-agent-chat.svg)

**Example:**: Customer on an e-commerce website starts entering a question about a product. A specialised chat-bot replies with a detailed response. An input box allows the customer to respond with a follow-up question.

**Mimicks:** üí¨ Colleague, mentor, customer service, etc. communicating with the user via chat.

## Pattern: Multi-agent chat

Multiple chat bots appear in the same chat window. Each bot has a different persona and perspective, and only contributes where applicable.

By splitting AI responses among multiple bots, rather than just one, it&#39;s easier for the user to mentally divide the AI output into different &quot;buckets&quot;.

Also, because this is analagous to real human-human team-work, it&#39;s intuitive for people.

Users can address individual bots by name, to ask for further assistance on a specific topic or aspect covered by just that bot.

![Example: specialised multi-agent chat bots embedded in a financial advisor tool](/images/articles/ai-patterns/ai-ui-patterns-multi-agent-chat.svg)

**Example:** Financial advisor tool for recommending products to customers. Agents representing analysts and compliance each offer a perspective. The advisor uses these insights to prepare for a meeting with the client.

**Mimicks:** üë≠ Group of people working together, such as a team meeting.

## Pattern: Fill-in-the-blanks

A stencil is displayed, with some areas for user input and some ares for AI generated content.

As the user fills in the inputs, the AI uses contextual information to generate more of the content. User and AI both work together until the full output has been generated.

![Example: fill-in-the-blanks AI for CV editing tool](/images/articles/ai-patterns/ai-ui-patterns-fill-in-the-blanks.svg)

**Example:** Writing a CV for a job. You start to fill in work history items. The AI suggests additional bullet points, which you accept or refuse. The AI suggests shorter more focussed descriptions and word removal, which you accept or refuse.

**Mimicks:** üìà Collaborative whiteboarding with colleagues (virtually or physically), collaborative card sorting exercises with a team.

## Pattern: Nudging controls

A &quot;work in progress&quot; is displayed in the center while command-buttons for &quot;nudging&quot; are displayed around the edges or off to the side. By clicking the buttons, you can ask the AI to change the work along some dimension.

![Example: nudging controls for AI-assisted logo editing tool](/images/articles/ai-patterns/ai-ui-patterns-nudging.svg)

**Example:** 3D image manipulation. We ask the AI to make the shape more or less rounded, more or less flat, etc.

**Mimicks:** üí∫ Pairing with a designer, where the designer is tweaking this or that based on your input.

## Further reading

- [_Exploring Generative AI_](https://martinfowler.com/articles/exploring-gen-ai.html) by Birgitta B√ñCKELER
- [_Artificial Intelligence: A Modern Approach_](https://aima.cs.berkeley.edu/) by Stuart RUSSELL, Peter NORVIG
- [_Co-Intelligence_](https://www.penguin.com.au/books/co-intelligence-9780753560778) by Ethan MOLLICK
</content>
  </entry>
  

  <entry>
    <title>Manual testing</title>
    <link href="https://conwy.co/articles/manual-testing" />
    <updated>2024-09-13T00:00:00.000Z</updated>
    <id>manual-testing</id>
    <content xml:lang="en" type="html">While automated testing methods have been established for a long time in the software development process (e.g. unit, integration and end-to-end tests), relatively less attention has been paid to manual testing.

However manual testing is far from &quot;dead&quot;. Software developers still routinely verify their work by using products manually. Further, developers are usually required to take responsibility for the end-to-end functioning of their software, not just writing quality code and passing unit tests. They are usually encouraged not lean too heavily on QA.

In this article, I will:

- Review the distinct benefits of manual testing.
- Present a real-life scenario where manual testing adds value.
- Provide guidance for structuring your manual testing, so you can get maximum benefit from it.

## Benefits of manual testing

Manual testing allows you to achieve certain specific goals which may not be available through automated testing:

- Discover ***actual behaviour*** ‚Äì how the system ***currently*** to behaves at runtime. (This information is not always readily available by other means.)
- Determine ***intended behaviour*** ‚Äì how the system is ***expected*** to behave. (Also not always readily available.)
- Perform testing ***immediately*** ‚Äì without setting up test frameworks, etc.
- Perform testing in ***any environment*** you can access ‚Äì not just local or QA environments.
- Put yourself ***in the end-user&#39;s shoes*** ‚Äì letting you experience the system as an end-user would.
- Verify ***complex, lengthy workflows*** ‚Äì which might be too difficult to automate. For example, complex interactions between an end-user and the system or complex data processing activities on the backend.

As with any activity, manual testing can offer maximal benefit when performed in a structured manner.

In my experience this involves:

* Writing clear, structured test cases - e.g. heading, description, numbered steps with actions and expected results
* Organising test cases for rapid retrieval, by using consistent naming and tags
* Sharing test cases within the team/organisation, so others can leverage them

## Example of a test case

Here&#39;s a simple example of a test case involving a user logging in:

```
user_can_login.md:

# User can login

Users who have an account should be able to log in.

## Steps

1. Go to the homepage
2. Click the login button
3. Expect that the login screen is shown
4. Enter username
5. Enter password
6. Click login screen submit button
7. Expect that you are shown logged in, in the header section
```

Notice we have a brief heading and description, followed by neatly numbered steps.

Steps can be:
* ***Actions*** (e.g. &quot;click the login button&quot;) or
* ***Expectations*** (e.g. &quot;expect that the login screen is shown&quot;)

This format allows us to quickly follow the steps of the test case (actions) and know what to look at to determine whether the test passed or failed (expectations).

## Scenario: critical fixes for a startup

A realistic scenario might make it easier for you to see how manual testing can help.

Imagine you begin work as a software engineer at a rapidly growing startup, building a complex product with many user flows.

You are assigned to work on the sign up experience. Users provide various personal details such as their country of residence. Based on these, the system provides various prompts then accepts payment.

You are given your first development task:

&gt; &quot;Please fix the flow for Japanese customers. They are getting stuck at the point where they submit their personal details, but before they have paid for the product.&quot;

This is based on direct customer contact. No one in the company can tell you exactly what &quot;stuck&quot; means or in exactly which part of the flow this is occurring.

There is also minimal unit test code, code quality is not good and there&#39;s little documentation. Remember, it&#39;s a fast-growth startup ‚Äì they don&#39;t have the same time and resources as a more mature company.

How would you go about solving this? Your approach might look like this:

1. You go through the flow manually, simulating a Japanese customer (perhaps setting your browser location to Japan).
2. As you go, you write down the steps you are taking, such as which details you entered, which buttons you clicked, etc. (This makes it easier to keep track of what you&#39;re doing, in case you need to restart the process). 
3. You find the exact point where the system is stuck - the submit button on screen 5 doesn&#39;t do anything.
4. Examining the requests/responses, you discover that the system skipped the collection of the user&#39;s driver licence details, which are required for customers in certain countries, including Japan, causing an underlying API call to fail if not provided.
5. Your verify this requirement with Backend engineers and the Product owner. Now you know what the fix is: you need to enable drivers licence details collection for Japanese customers.
6. You make the fix in the relevant part of the code-base.
7. Testing your work manually, you realise this data can be collected  earlier in the sign-up flow, with a skip option given for customers who don&#39;t have the details on hand. This will be a nicer user experience and increase the number of potential customers in the sales funnel.
8. You complete all your changes, cover them with a unit test, save your manual testing steps in a markdown document (linked to from the pull-request) and push your changes.
9. Once in Prod, you do a quick verification and see that everything works as expected. 
10. You can now report to the team that your task is completed with (hopefully) zero bugs!

Notice how documented manual testing helped you to solve this problem:

- You found the actual error by manually going through the flow (step 1).
- You kept track of your testing by writing down the steps, allowing you to quickly and efficiently repeat your test efforts whenever needed (steps 2, 7, 9).
- You easily verified your work in Prod (step 9).
- You empathised with the end-user and even found an opportunity to improve their experience as well as the onboarding rate (step 7).
- You added value to the team by documenting your manual testing steps (step 8).

As we&#39;ll soon see, this is only the beginning of the benefits! 

## Tagging your test cases

Tagging can be a powerful way of making your whole test case collection searchable.

Suppose every time you refer to the login screen in your Markdown files, you use the exact phrase: &quot;login screen&quot;. Perhaps wrap it in brackets: &quot;(login scren)&quot;.

Now this exact phrase is searchable, via a simple find-in-files in your text editor. By searching for the string &quot;(login screen)&quot; you can find every test case involving that screen.

For example, your search might yield the following results:

* `user_can_login.md`
* `user_can_recover_forgotten_password.md`
* `user_cannot_login_with_wrong_credentials.md`
* `user_can_login_from_another_country.md`
* `user_can_login_with_a_linked_google_account.md`

This gives you powerful new capabilities such as:

* **Regression-testing** - checking various test cases, in case your change might have broken something.
* **Exploratory-testing** - observing how the application behaves in various scenarios, generating ideas for improvement or uncovering hidden bugs.
* **Determining which unit tests to write** - to boost test coverage in a critical area of the application.

## Test data 

Suppose a feature you want to test relies on certain data existing in the system beforehand.

For example, you might need a certain kind of user account, such as a user who has their country set to Japan.

You could create a test user in your testing environment - `hiroshi@yompail` ‚Äì and save it in your test case under a &quot;Test data&quot; heading.

```
user_can_login.md:

# User can login

## Steps

1. Go to the homepage
...

## Test data

- User: hiroshi@yopmail.com / P@ssw0rd
```

## Results and artifacts

It can be very useful to know the full list of dates/times when you ran your test and what the result was on each run.

These can be added to a &quot;runs&quot; section of the test case file.

```
user_can_login.md:

# User can login

## Steps

1. Go to the homepage
...

## Runs

| Date/time               | Result    |
| ----------------------- | --------- |
| 2024-10-01 9:00 AM      | Succeeded |
| 2024-09-04 10:00 AM     | Failed    |
```
 
How might this be useful?

* **Spotting a pattern in failures** can indicate a systemic problem, such as insufficient compute resources or code quality issues with a particular part of the code base.
* **Correlating failures with code changes** can narrow your version control system search. For example, if you know the failure happened within the last week, you can limit your search changes made within that timeframe.

When with manual testing, it is common for engineers to capture artifacts of their work, such as screenshots, screen recordings and copies of log output. These serve to demonstrate work done, prove that things worked correctly at a certain date/time and capture additional information that could help identify additional problems or improvement opportunities.

Artifacts from manual tests can be organised alongside test cases, using a structured folder naming system.

I have found it best to keep artifacts in folders named after the test cases and test run dates from which they were generated.

Here&#39;s an example:

- `/test_cases`
  - `user_can_login.md`
  - `user_can_recover_forgotten_password.md`
  - ... etc
- `/test_artifacts`
  - `/user_can_login`
    - `/2024_10_01_9_00_AM`
      - `Screen Recording 2024-10-01 at 9.01.55 am.mov`
      - `Untitled2.png`
  - `/user_can_recover_forgotten_password`
  - ... etc

## Manual testing workflow

You can make manual testing a regular, consistent part of your workflow. As you strengthen this habit, your work quality and overall knowledge of the system should improve.

Here are some ideas:

- Write a test case at the beginning of working on a major feature.
- Include or link to the test case in the task tracking system.
- Perform a test case of every major feature you deliver, writing a test case if one doesn&#39;t already exist.
- Include or link to your test case from every pull request you submit.
- Keep all your test cases in a shared knowledge system, such as your project&#39;s wiki.
- Copy relevant parts of a test cases in chat conversations about a feature or bug.

## Manual testing tools

There are a range of software tools to help you write and manage test cases.

- [Testmatic](/projects/testmatic). Shamless plug ‚Äì I built this! It includes a web-based UI and CLI and saves everything to Markdown.
- [Azure Test Plans](https://azure.microsoft.com/en-us/products/devops/test-plans). This one has a nice web-based UI and integrates with the Azure suite.
- [DoesQA](https://does.qa/test-automation/codeless-vs-code). An interesting product that apparently allows you to write &quot;codeless&quot; but runnable tests.

## Conclusion

Manual testing offers distinct and powerful benefits, not offered by automated testing, such as understanding and representing current and desired system behaviour, making fast progress in challenging environments with limited documentation and test coverage, verifying changes in multiple environments, verifying complex workflows and empathising with end-users.

Structuring your manual test efforts compounds these benefits: you can quickly locate related tests (enabling regression and exploratory testing), ease your test efforts (using test data) and keep track of test results (helping you identify patterns in failures or find the root cause of an issue).

## Further reading

These resources inspired this article:

- [_Software Testing - A Craftsman&#39;s Approach_](https://www.routledge.com/Software-Testing-A-Craftsmans-Approach-Fifth-Edition/Jorgensen-DeVries/p/book/9780367767624) by Paul JORGENSEN</content>
  </entry>
  

  <entry>
    <title>Mindfulness</title>
    <link href="https://conwy.co/articles/mindfulness" />
    <updated>2024-09-13T00:00:00.000Z</updated>
    <id>mindfulness</id>
    <content xml:lang="en" type="html">&gt; ‚ÄúLet all those beings which exist --
&gt; without enemies, without obstacles, overcoming their grief
&gt; and attaining happiness, be able to move freely,
&gt; each in the path destined for them.‚Äù
&gt; ‚Äì BUDDHAGHOSA ‚Ä¢ Visuddhimagga

&gt; ‚ÄúWe are what we repeatedly do.‚Äù
&gt; ‚Äì ARISTOTLE

&gt; ‚ÄúMindfulness is not merely a concept or a good idea. It is a way of being.‚Äù
&gt; ‚Äì John KABAT-ZINN

During my most recent sabbatical I experimented with mindfulness-meditation.

Starting with 10 minutes then moving up to 20 minutes per day, I sat in an small spot outdoors, observing my surroundings and well as my body and mental activities.

I&#39;m glad to report that, with a little persistence, this has now become a firm daily habit and something that I quite look forward to!

During this period I was reading a number of books that seemed relevant, including:
- _Full Catastrophe Living_ by John Kabbat Zinn
- _Zen Mind, Beginner&#39;s Mind_ by Shunryo Suzuki
- _The Tao Te Ching_ by Lao Tzu (various translations)

I had some interesting experiences and thoughts, which I thought I&#39;d share.

**Transformed experiences.** When I experienced sounds and sights, I observed my (rather overactive) mind reacting to them. I observed a tendency to distinguish and label objects, animals, people, etc. Over time I gently nudged myself away from labels, focussing more on the wordless experience of these phenomena. I also found my experience broadening to encompass the _overall atmosphere_ of the spot ‚Äì the &quot;total combination&quot; (if you will) of all these phenomena. I found this practice began to subtly transform my experience of the environment, especially over time and repetition. I began to feel a greater sense of unity with the world. I don&#39;t think this was necessarily due to to any intellectual realisation, but more from developing a habit of ***not distinguishing*** myself from my surroundings. 

**Enhanced memory.** I noticed an ability to recall things more easily ‚Äì from facts and figures I&#39;ve been learning in various studies to even imagery from dreams. I&#39;m not sure whether this is attributable to my meditation practice, but at least it seems to have coincided with it.

**Beauty.** I catch myself more often being struck by beauty. Not only sunsets but also the beauty of peoples&#39; dress, of language, of various animals (magpies, various insects) and plants, and also landscape features like hills. I don&#39;t experience the beauty of these separately (though it probably seems that way because of how I write about them), but more often together as part of the same scene. It&#39;s a great joy!

**Observing non-critically, withholding judgement.** I&#39;ve been practicing a more mindful approach generally. For example, when reading code or someone&#39;s writing, I try to practice a small hesitation. I hold back from assuming that my initial understanding is correct and re-read the material to check if I really understood it. I&#39;m hopeful that this will help me to avoid mistakes in future and also make the reading experience more engaging (both for code and writing).

**Seeing obstacles as opportunities.** Initially I had to somewhat coax myself into mindfulness. I have been mindful many times throughout my life, but carving out a specific time in my day just for this practice took a bit of persistence. There is a saying that no meditation is &quot;bad&quot; ‚Äì what matters most is that you simply do it. In this spirit, I tried to look at my effort (and the resistance to it) as an obstacle leading to an opportunity ‚Äì the opportunity to increase awareness and inner peace.

**Moving quickly without rushing.** In the _Catastrophe_ book, Zinn refers to: &quot;being aware even when moving quickly&quot; and suggests: &quot;shift your awareness ... to a sense of your body as a whole moving through space&quot;. I practiced this awareness during busy moments, such as shopping and commuting. It really took the stress out of these and at times made them quite enjoyable.

I hope you found these insights interesting and that perhaps I&#39;ve whetted your appetite for practicing mindfulness ‚Äì in whichever way works best for you.

Namaste!</content>
  </entry>
  

  <entry>
    <title>What I learned studying Calculus</title>
    <link href="https://conwy.co/articles/studying-calculus" />
    <updated>2024-09-13T00:00:00.000Z</updated>
    <id>studying-calculus</id>
    <content xml:lang="en" type="html">&gt; ‚ÄúI never regretted the time I spent on history and on math. Math sharpens your mind, history gives you some idea of your limitations and what‚Äôs going on in the world.‚Äù
&gt; ‚Äì Bjarne STROUSTRUP ([Video](https://www.youtube.com/watch?v=-QxI-RP6-HM))

Recently I completed [*Introduction to Calculus*](https://www.coursera.org/learn/introduction-to-calculus) on Coursera.

My motivations were:

* To improve my foundational math skills, enabling a better understanding of computing foundations and machine learning.
* To test and improve my learning skills by studying a less familiar field outside comfort zone.
* Ok... the real reason... to gain entry to the Sydney University B. Adv. Comp. program! üòÑ

The course has been fascinating and engaging. It&#39;s not only introduced me to Calculus but also helped me to brush up on basic arithmetic and algebra where I discovered I had serious gaps. [Prof David Easdown&#39;s](https://www.sydney.edu.au/science/about/our-people/academic-staff/david-easdown.html) regular participation in the forums was valuable gift to us students!

Some reflections in no particular order:

* **Understanding fundamentals.** The hard part often isn&#39;t the highest level concepts, but the foundations you need in order to (properly) understand and use them. I had to understand concepts such as *completing the square*, *long division of polynomials* and *derivatives*, not only theoretically, but well enough to apply them correctly to solve an unexpected problem. I&#39;ve [commented](https://news.ycombinator.com/item?id=40083488) about this on HackerNews.
* **Benefits of drawing, visualising.** Using a high quality notebook and pen helped with enjoyment and motivation to study! Fleshing out problems on paper, down to the fine details, helped me to see how the pieces of a problem fit together, which was quite fascinating. 
* **Geometric analogies.** I found it incredibly helpful when David presented an idea *geometrically*. Simple shapes such as triangles are intuitive to many learners. And the [Theorem of Pythagoras](https://en.wikipedia.org/wiki/Pythagorean_theorem) is now widely understood. The curve sketching exercises and Riemann sum rectangles were memorable examples but there were so many other cases where David made the idea clearer by some kind of geometric metaphor.
* **Parts vs. whole.** I found that solving a non-trivial math problem typically requires also understanding all the parts of the problem. (This is in contrast to my usual profession, software development, where many problems can be solved using &quot;black-box&quot; solutions such as libraries, without understanding their internals.) This experience of the parts/whole adds a dimension to my understanding of problem solving generally.
* **Focussing on mistakes.** Trying out the examples and practice tests and observing what I got wrong helped me narrow down specifically where my knowledge gaps lay and then actively correct them, which also helped improve my long-term memory of the concepts. (This way of thinking is also helping me in other areas of life beyond math.)
* **Different learning methods at different stages.** I found different learning techniques useful at different stages of the course. Early in the course, I found repeated practice and deep thinking most useful. Midway through, I found sketching and visualisation more productive. Late in the course, I found all the above useful, plus reading various additional sources such as books.
* **Thinking deeply about small things.** Prof David mentioned this in one of the videos - such a powerful insight! Sometimes on a walk or another repetitive activity I turned over an idea in my head to try and understand it in its essentials (e.g. I did this a lot with Leibniz notation, trying to understand it at some deeper level than words).
* **Temporarily suspending understanding.** Introspectively, I found that getting stuck (what people call &quot;writers block&quot;, &quot;analysis paralysis&quot;, etc) often happens when I&#39;m trying to understand an interconnected complex of ideas and am uncertain how to break them down. Here the usual approach of &quot;divide and conquer&quot; or breaking down the problem into smaller pieces doesn&#39;t work because I don&#39;t know how or according to what criteria to break them down. One way to proceed is to temporarily suspend trying to understand some parts so that I can focus on others. I&#39;ll treat them as small &quot;black boxes&quot; so that I can skip learning them. This enables me to move on to other problems and not be blocked.
* **Concepts vs. application.** Abstract conceptual knowledge is not the same thing as knowledge that can be applied. I could read a book on Calculus without really understanding it, but doing the practice tests in this course was better. After this course it&#39;s more likely that I&#39;ll be able to see a problem, see where Calculus might help, identify/name the components and then try to solve it using appropriate methods.
* **The power of good notation.** Prof David mentioned this in one of the videos on Leibniz notation. Amazing, rich insight. This immediately inspired ideas that I&#39;ve been translating into my software development practices. Representing a problem differently can open the way to unexpected solutions. This inspired my [code selectors](/projects/codeselectors) project.
* **Mathematics as a language.** Sometimes thinking of a mathematical idea as language or &quot;description&quot; helped me understand it. Rather than trying to understand a concept&#39;s full meaning immediately, I try to think of it as a just label ‚Äì in object-oriented programmer-speak, a &quot;class&quot;. Then I gradually add &quot;properties&quot; or &quot;attributes&quot; to that label as my understanding increases. For example, I found this method useful for understanding [Euler&#39;s number](https://en.wikipedia.org/wiki/E_(mathematical_constant)). Rather than thinking of it as a particular number (which stumped me for a while) I found it better to think of it as the *criteria* for some number which I don&#39;t yet know. In SQL programming terms: a `SELECT` statement, with multiple `WHERE`/`AND` clauses, which yields a single scalar result which is _e_. In these terms, *e* is &quot;the number which yields has a slope (rise over run) of 1 at 1&quot;, etc. Thinking this way helped me reason about the use of *e* in graphs.
* **Directions for further study.** I greatly appreciated David&#39;s frequent references and pointers to areas for further study. Explaining the *proofs* of a method, rather than just the method itself, is one example ‚Äì I could use this knowledge for [formal verification](https://en.wikipedia.org/wiki/Formal_verification). Referencing more advanced concepts such as series expansions is another example. This makes me curious and motivates me to study the field further.

I started this course with some trepidation and had many challenges along the way, but I feel a great sense of satisfaction after completing it.

Now I can&#39;t wait to jump into my next learning challenge: [Linear Algebra](https://www.coursera.org/learn/introduction-to-linear-algebra)!

## Further reading

Books that helped me along this journey:

- [_Precalculus: Mathematics for Calculus, Seventh Edition_](https://www.amazon.com.au/Precalculus-Mathematics-Calculus-James-Stewart/dp/1305071751) ‚Ä¢ James STEWART, Lothar REDLIN, Saleem WATSON

</content>
  </entry>
  

  <entry>
    <title>Streamlining code reviews</title>
    <link href="https://conwy.co/articles/code-reviews" />
    <updated>2024-09-13T00:00:00.000Z</updated>
    <id>code-reviews</id>
    <content xml:lang="en" type="html">For myself and others, I&#39;ve found that code reviews can be a challenging part of being a software developer.

There may be many code changes to go through, many possible mistakes to be found and limited time to find and communicate them. Even before considering mistakes, the sheer amount and complexity of the code and changes can feel overwhelming ‚Äì you don&#39;t know where to begin. It often feels like code review is chaotic and random, and it&#39;s down to luck whether or not it will be constructive.

In this article I want to share my code review process, which I&#39;ve evolved over time, in response to the pressures mentioned above.

These are not necessarily based on hard evidence such as data and statistics, but more of a grab-bag of potentially useful ideas sampled from a range of different work environments, development stacks and teams, over many years.

Broadly, there are three practices I follow, in order, for each code review:

1. **_Context_** ‚Äì gain a high level understanding of the context surrounding the change, by checking the description, commit messages, task and any other documentation, and/or by asking the author
2. **_Scan_** ‚Äì scan all of the code in the change, observe any questions or issues that come to mind, try to answer them on my own, otherwise leave a comment
3. **_Checklist_** ‚Äì make a final pass of all the code, this time against a checklist, and call out minor and major issues

Let me go into a bit more detail on how I perform each of these.

## Context

It&#39;s difficult to meaningfully assess a code change without understanding its **_context_**.

Context includes: what problem or requirement the code change intends to solve and how it fits into the broader context of the system.

Without context, the code may either look superficially correct, leading you to approve it too quickly, or it may look very odd, generating innumerable questions to the author. Conversely, when you **_do_** understand the intention and context, a code change becomes much easier to understand and more intelligible.

So I&#39;ve found it useful to learn more about the context if I&#39;m unclear.

Here are some methods I use to learn more about the context:

- Read descriptive notes and commit messages for the change, if any.
- Look up the task associated with the code change and read description and comments there.
- Look up code files related to the change and examine the code there.
- Look up the **_history_** of the files associated with the change, look up the tasks associated with that history, read the descriptions and comments.
- Search internal information sources (chat channels, wiki, etc.) using keywords found in the code.
- Ask the author of the change directly for context.
- Look up the authors of the files associated with the change in version control history and ask them directly for context.

While the above might seem time-consuming, I&#39;ve found it possible to fairly quickly improve my knowledge, even within minutes, by just picking a few of the relevant methods and applying them.

For example, if I already work closely with the author involved, a simple message asking for context often gets a quick reply. Or if the code changes are attached to tasks, e.g. via task codes, it&#39;s usually possible to access the task with just a couple of clicks, and then read it within a few minutes.

I believe this &quot;context hunting&quot; is usually worth the effort. It&#39;s not only about understanding the code change you&#39;re looking at. The benefits of contextual knowledge compound over time. Initially you might spend, say, 10 minutes reading and digesting contextual information, but eventually the time spend can approach 0, as you develop a systematic understanding of the whole system. That systematic understanding is highly valuable in all kinds of ways, not only for code review. It can help you to succeed in your own projects within the organisation and even help you to make the case for new projects and initiatives.

## Scan

After gaining context, the next step is to scan the code, getting a &quot;big picture&quot; view of how it hangs together.

I&#39;ll often check out the change locally, open some of the files in my IDE, and use the navigation tools in the IDE to figure out what sequence of calls are being made, what data are being passed, etc. If it&#39;s a complex network of calls, I might spend a few minutes sketching an [execution flowchart](/articles/visualising-execution-flows).

I also survey the content of new code added, looking at key variables, control-flow structures (conditionals, switches, loops, etc), and getting an overall grasp of what the code does.

At this point I may already have questions or issues for the author, and if so, I won&#39;t hesistate to leave a few comments.

My comments will usually be in the following form:

```
[Priority]: (Message)
```

This helps the author to understand my intent and prioritise which comments to reply to.

For example, if it&#39;s a minor issue, which shouldn&#39;t necessarily block merging, I&#39;ll write something like this:

```
[Minor]: `.forEach(expandSection)` might be more concise here.
```

But if it&#39;s a question, I&#39;ll write:

```
[Question]: Should this section be hidden for users without permissions?
```

And if it&#39;s a major issue, I&#39;ll write:

```
[Major]: Should include an authorization check here.
```

## Checklist

By this point I&#39;ll have a pretty solid understanding of the change.

Now it&#39;s a good time to run through a code review checklist and see if I missed anything significant.

Because I broadly understand the change as a whole, even with a large checklist of 50 items, it&#39;s possible to quickly scan the checklist and pick out only the items that apply to the code.

For example, suppose one of the items in my checklist is:

- Query keys should be appropriately unique

After scanning the change, I will already know whether or not the change includes any query keys at all. If it does not, then I can immediately skip this step.

On the other hand, if the change **_does_** contain enums, then I will know to check the following item in my checklist:

- Enum values should match keys

Where does this checklist come from?

I usually build a unique checklist for each project I work on. As initial inputs to the checklist, I analyse the codebase I&#39;m working on and read any technical documentation, such as coding standards.

Subsequently I will add new items to the checklist, based on comments others leave on my change submissions, technical discussions with team members and general observations.

Additionally, I&#39;ve built up a pool of coding standards and best practices over my time as a developer. Some of these you can find documented in my article, [Towards zero bugs](/articles/towards-zero-bugs). I plan to publish a comprehensive list of them in a future blog post.

This checklist isn&#39;t only useful for reviewing others&#39; work ‚Äì I use it on my own changes as well. By anticipating feedback and addressing it earlier, my code will already be of higher quality by the time it reaches the screens of others. This reduces the review workload on other engineers and improves my reputation within the team.

## Conclusion

Code reviews are an integral part of modern software engineering.

At a team level, they&#39;re a great way to maximise code quality and ensure a shared understanding and knowledge of the code and systems.

At an individual level, they&#39;re useful for understanding as much of the code as possible, both at a high level and a detailed level. This improves the quality of my own work and increases the likelihood of success in my current work and new initiatives within the organisation.

Having a normalised process for performing code reviews helps make them easier and more fun. It also improves the quality of the feedback and, long-term, the code base.

## Further reading

These books inspired this article:

- [_Software Engineering at Google_](https://www.kobo.com/au/en/ebook/software-engineering-at-google) by Titus WINTERS, Tom MANSHRECK, Hyrum WRIGHT
</content>
  </entry>
  

  <entry>
    <title>Visualising execution flows</title>
    <link href="https://conwy.co/articles/visualising-execution-flows" />
    <updated>2024-09-13T00:00:00.000Z</updated>
    <id>visualising-execution-flows</id>
    <content xml:lang="en" type="html">Anyone who has spent some time developing software knows writing new code is but a small part of the job. At least as big, perhaps bigger, is **understanding the existing code**. And that includes understanding the **runtime behaviour** of that code!

I often found myself having to understand a complex cluster of code modules, entailing many function calls being made and many data types being passed and returned.

To properly understand the behaviour of the code, I needed to see a whole flow together at once so I could reason about it. I needed to somehow visualise it, e.g. by listing out the function calls in a text editor or maybe drawing a diagram on a piece of paper or in a diagramming application.

After doing this quite a few times, I have started to evolve a more consistent and powerful format, one which is text-based (and so, easy to work on in a standard text editor) but can also be converted to a visual flowchart using a tool called Mermaid.

In this article I want to describe this format and the reasoning behind it.

Execution flow notations can be useful in understanding an existing code-base, troubleshooting bugs, communicating with other team members and for solution design.

But first some background...

## What do I mean by &quot;execution flow&quot;?
What&#39;s an execution flow?

It&#39;s helpful to define the concept of &quot;execution flow&quot;.

I&#39;m referring to the path that the runtime will take through the code as it executes the code during a real-life use case.

You should not confuse this with a more specific term: **_call stack_**. Since a flow can include multiple function calls in sequence, each producing its own distinct call stack, a flow can include multiple call stacks. Much of the complexity of an execution flow is precisely that calling of multiple functions and the passing of data to them and returning of data from them. So &quot;call stack&quot; is too narrow a term to cover what I&#39;m trying to describe.

On the other hand, you should also not confuse this with a more general term such as **_abstract syntax tree_** or &quot;code structure&quot;. We are not describing the code as a whole, but just one path of possible execution of the code. Any piece of code that has one or more conditionals (e.g. `if` or `switch` statement, etc.) will execute differently depending on how those conditionals evaluate. For the same code, different lines might execute depending on the situation (e.g. depending on external state of some kind such as a database, web-service, system clock, etc.). Thus one code base can support multiple execution flows.

## Example of an execution flow
An example

Let&#39;s use a hypothetical example ‚Äì handling a user login on a Java backend.

```java
class LoginResource  else 
  }
}
```

Can you spot the two execution flows in this code?

### Flow 1 - Logging in successfully

First, we have flow when the user&#39;s credentials are valid.

Here&#39;s that code example again, with the relevant lines highlighted:
 

   else 
  }
}
`}
  


1. Inside `login()`, the `if` condition calls `UserAuthProvider::checkUserCredentials`, passing user credentials.
2. `UserAuthProvider::checkUserCredentials` returns `true`.
3. Execution proceeds into the `then` block.
4. We call `SessionProvider::setCurrentUser`, passing user credentials.
5. We return `Response`, passing success parameters.

Notice that this isn&#39;t just a single call-stack, as there are actually two method calls in this flow, each of which will generate its own call stack.

1. `UserAuthProvider::checkUserCredentials`
2. `SessionProvider::setCurrentUser`

### Flow 2 - Failure to log in

What if the user credentials are _not_ valid and `isValidUser` returns `false`?

That would be a separate execution flow.

Here&#39;s the code example once more, with the relevant lines highlighted:


   else 
  }
}
`}
  


1. Inside `login()`, the `if` condition calls `UserAuthProvider::checkUserCredentials`, passing user credentials.
2. `UserAuthProvider::checkUserCredentials` returns `false`.
3. Execution proceeds into the `else` block.
4. We return `Response`, passing failure parameters.

## IDE tools for determining execution flow
IDE tools

So how to we figure out how our code flows in the first place?

We can, of course, just read the code, open various files as needed, and try to follow along in our head.

Thankfully we also have automated tools to help reduce some of the tedium. You&#39;ll likely be familiar with these:

- **_Go to definition_** - we can select a reference (function, class, variable, etc) and be taken to its original definition
- **_Find references_** - we can select a definition (function, class, variable, etc) and pull up a list of all points in the codebase which reference the definition

Different IDEs name these differently, but most mainstream IDEs have them in one form or another, including IntelliJ IDEA, VSCode, Visual Studio and xCode.

![Screencast of a developer using Go to definition tool in IDEA](/images/articles/visualising-execution-flows/visualising-execution-flows-demo-1.gif)

For example, in the code sample given previously, we might use _Go to definition_ to locate the class whose `login` method is being called.

1. Go to the `LoginResource` class and its `login` method.
2. Right-click the `isValidUser` call and select &quot;Go to definition&quot;.
3. Observe that it is defined in the `UserAuthProvider` class and its `isValidUser` method.
4. Go back the `LoginResource` class and its `login` method.
5. Right-click the `setCurrentUser` call and select &quot;Go to definition&quot;.
6. Observe that it is defined in the `SessionProvider` class and its `setCurrentUser` method.
7. Go back the `LoginResource` class and its `login` method.
8. Observe that a new `Response` object is constructed.



We might want to find out where else the `UserAuthProvider::isValidUser` method is called.

Supposing there was a `RegisterResource` class having a `register` method, as shown below:



   else 
  }
}
`}
  


Then we might locate this piece of code by using the _Find references_ tool:

1. Go to the `UserAuthProvider` class and its `isValidUser` method.
2. Right-click the `login` method and select &quot;Find usages&quot;.
3. Observe that it is called in the `LoginResource` class, in its `login` method.
4. Observe that it is also called in the `LoginResource` class, in its `register` method.
5. Observe that a new `Response` object is constructed.

## Describing execution flows with text
Describing with text

Suppose we wanted to make some notes of the execution flows we discovered. Maybe there are too many for us to easily memorise. Perhaps we want to see them all in one view rather than scattered among many files.

Let&#39;s start with the first flow ‚Äì successful login:

```mermaid
LoginResource::login
  ---&gt;|userName,password| UserAuthProvider::isValidUser
  ---|true| LoginResource::login

  ---&gt;|userName,password| SessionProvider::setCurrentUser
  --- LoginResource::login

  ---&gt;|200,&#39;Login succeeded.&#39;| Response::constructor
```

Then the second flow ‚Äì successful login:

```mermaid
LoginResource::login

  ---&gt;|userName,password| UserAuthProvider::isValidUser
  ---|false| LoginResource::login

  ---&gt;|401,&#39;Login failed. Invalid credentials.&#39;| Response::constructor
```

And the final flow ‚Äì register:

```mermaid
LoginResource::register
  ---&gt;|userName,password| UserAuthProvider::isValidUser
  ---|true| LoginResource::register

  ---&gt;|200,&#39;Login succeeded.&#39;| Response::constructor
```

Now we can step back and look at all these flows together and see the bigger picture, e.g. how login and register both check user validity using `UserAuthProvider`, and how both instantiate the Response class with various constructor parameters.


  Sketching execution flows
  
  Observe that we don&#39;t have to cover the flows exhaustively, nor do we have to limit our coverage. We can cover just the parts of code that we are concerned with, based on our current goal, e.g. to solve the current bug or to learn more about a specific part of the code-base. We can make a kind of &quot;partial sketch&quot; of the parts of the execution flows that interest us.

  For example, we don&#39;t cover how login and register are called, and we don&#39;t cover which further calls are made by UserAuthProvider, SessionProvider or Response, if those parts of the code don&#39;t interest us.

  And if we are dealing with a bug in which isValidUser incorrectly returns false, we can focus more on isValidUser and which methods it calls.


## Format for notating execution flows
Notation format

Did you notice the textual format used in the previous section to notate the execution flows?

Let&#39;s deep-dive into that.

```mermaid
Class::methodCalling
  ---&gt;|parameters| Class::methodBeingCalled
  ---|return values| Class::methodCalling
```

- `Class::methodCalling` - the caller
- `---&gt;|parameters|` - execution flowing from caller to callee, with the parameters being passed in the call
- `Class::methodBeingCalled` - the callee
- `---|return values|` - execution flowing from callee back to caller, the value returned from the callee
- `Class::methodCalling` - the caller (again)

We can chain these together to notate a sequence of consecutive calls.

For example:

```mermaid
Class1::method
  ---&gt;|parameters| Class2::method
  ---&gt;|parameters| Class3::method
  ---|return values| Class2::method
  ---|return values| Class1::method
```

## Representing closures and indirect calls
Closures and indirection

Thusfar we&#39;ve use the `Class::method` format to reference the callers and callees. This should work reasonably well for classical OO code-bases written in Java, C#, Swift etc.

But what if we want to reference code in other ways, such as named closures, for languages written in Javascript, Typescript, etc.?

Here are some notations that could allow such structures to be referenced:

### Nested closure

foo/bar - Reference a closure witin another closure.



### Indirect call

-.-&gt; - References an indirect method call - a call which our code doesn&#39;t make directly, but causes to be made, such as calling `setTimeout` on a function in Javascript. It looks like a dotted line.

### Example in Javascript

Let&#39;s use an example ‚Äì a recursive Javascript function ‚Äì to put all these ideas together.

```javascript
function retry(action, times, count = 1) 
  }, timeout);
}
```

`retry` is a recursive function, which calls `setTimeout`, passing a closure. That closure executes. Depending on the number of times `retry` has called itself already (`time`), it may call `retry` again or simply do nothing, halting the recursion.

We can notate this execution flow, including the closure, using the nested closure, multiple calls, row/column and indirect call notations given above, in the following manner:

```mermaid
flowchart
  retry
    -.-&gt;|action, times=3| retry/handleTimeout
    ---&gt; action
    ---|false| retry/handleTimeout
    ---&gt;|action, times=3, 2| retry
    -.-&gt;|action, times=3, count=2| retry/handleTimeout
    ---&gt; action
    ---|true| retry/handleTimeout
    ---retry
```

## Visualising flows with Mermaid
Mermaid

Now the juicy part ‚Äì lets look at how this format can be instantly converted into a visual flowchat using Mermaid!

[Mermaid](https://mermaid.js.org/) is a free, open-source tool, which takes code written in a specific syntax and converts it into a diagram.

You can run Mermaid in the browser using [Mermaid Live](https://mermaid.live/), or if you prefer, you can download and run it locally using the instructions on the [mermaid-live-editor](https://github.com/mermaid-js/mermaid-live-editor) GitHub profile.


  Note: We&#39;ll need to add the keyword graph to the top of the text.
  Also, in these examples, we add numbered circular nodes (e.g. ---n1((1))) to indicate the order of execution.


The following is how our two earlier Java examples ‚Äì login success and login failure ‚Äì render in Mermaid:






And here&#39;s the Javascript example:



Notice that we&#39;ve added small numbered circles, indicating the order in which the calls occur. This makes the flow a bit easier to navigate.

Imagine this appearing in a Slack conversation:

It could potentially be easier to read and follow an execution flow diagram than to read paragraphs of text trying to describe in plain language the complex sequence of calls.



  Asynchronicity and concurrency

  Though we touched on async in the Javascript example with the setTimeout call, we haven&#39;t fully addressed the issue of describing asynchronicity or concurrency in execution flows.

  This is probably a fairly deep topic that deserves a dedicated article. However I have no doubt it can be represented diagrammatically, as long as a strict convention is adhered to.


## Isn&#39;t this just a flowchart?
Vs flowcharts

Yes, but it&#39;s a **_specialised_** form of flowchart, focussed on representing execution flow.

The flowchart directly maps to the code it represents, so it accurately and unambiguously conveys information about that code. At the same time, because it&#39;s not actually code, but a diagram, it allows us to more easily view and reason about the code in terms of execution flows specifically. We don&#39;t have to jump around between files, scroll up and down, etc. but can see a whole execution flow in one screen.

Also by establishing and adhering to a convention in how we represent callers, callees, parameters and return values, etc. this flowchart technique is re-usable across programming languages, codebases, business domains, etc. A similar versatility is found in UML, sequence diagrams and other kinds of specialised diagram formats.

## Why not use sequence diagrams?
Vs sequence diagrams

You might have seen diagrams similar to those described here, but laid out as sequence diagrams. Execution flows can certainly be visualised as sequence diagrams. A sequence diagram is arranged as a set of vertical columns connected by arrows, where each column represents a method and each arrow represents a call.



There are weaknesses of sequence diagrams, however.

- They present each method in a column, so we may soon run out of horizontal space, whereas flowcharts can flow **_down and across_**. Also, even for lengthy flowcharts, scrolling up and down is easier on most devices than scrolling side-ways.
- They may position the caller and the callee very far apart, so that the eye has to scan back and forth over a large distance to see the call, whereas flowcharts can more position the caller and callee closer together, making scanning easier.

For these reasons, I find the flowchart format more appealing.

## Are there tools that generate execution flowcharts automatically?
Automatic generation

Surprisingly, not really.

For dependency visualisation, I found a [few](https://marketplace.visualstudio.com/items?itemName=sz-p.dependencygraph) [interesting](https://marketplace.visualstudio.com/items?itemName=lilinhao.vscode-pylonn) [plugins](https://marketplace.visualstudio.com/items?itemName=CodeLogic.vscodecape) for VSCode, and also experimented with [IDEA&#39;s dependencies analysis](https://www.jetbrains.com/help/idea/dependencies-analysis.html) tool.

However, all of these tools are focused on reporting **_compile-time dependencies_**, which are a different kind of thing to **_execution flows_**.

Dependency graphs of course help us to understand how code is structured, but they don&#39;t give us the full picture of which parts of that code execute in which order at runtime. For that, we really need execution flows.

Theoretically any tool that could automatically report execution flows would need to be able to analyse the code in terms of its expected execution at runtime. The tool might, like a debugger, execute the code, in order to determine the flow of control, e.g. where the flow of control depends on some state which can only be discovered at runtime. Or it could statically analyse the code to determine all possible flows and generate a report of all of them.

It&#39;s beyond the scope of this article to look into how such a tool could be developed, but it&#39;s something I&#39;m interested in looking into and perhaps even undertaking myself.


  Use of ChatGPT
  
  My experimentation with ChatGPT yielded promising results.
  
  The LLM (Large Language Model) tool was able to generate a flowchart with labels in plain-English in both ASCII and Mermaid formats.
  
  The flowchart did accurately follow the flow of the code. However, it did not use the format I described above, which is intended to directly map to elements in the code (function names, variable names, etc).
  
  With some more training of ChatGPT, more detailed prompts or a more customised LLM tool than ChatGPT, perhaps it will be possible in the near future for a chat-bot to generate execution flows automatically. That would be cool!


## Conclusion

This article has outlined a format for describing execution flow, which can be used to visualise and understand how parts of a codebase execute at runtime (and generate diagrams).

This understanding can help to diagnose bugs/errors, determine the best points at which to change the code, estimate how long changes might take, and no doubt many other use cases.

I hope you find it useful!

## Further reading

These books inspired this article:

- [_The Pocket Guide to Debugging_](https://jvns.ca/blog/2022/12/21/new-zine--the-pocket-guide-to-debugging/) ‚Ä¢ Julia EVANS
- [_UML Distilled_](https://www.martinfowler.com/books/uml.html) ‚Ä¢ Martin FOWLER

</content>
  </entry>
  

  <entry>
    <title>Front end observability</title>
    <link href="https://conwy.co/articles/front-end-observability" />
    <updated>2024-09-13T00:00:00.000Z</updated>
    <id>front-end-observability</id>
    <content xml:lang="en" type="html">I&#39;ve been reading the book [_Observability Engineering_](https://info.honeycomb.io/observability-engineering-oreilly-book-2022) by Charity Majors and thinking about how to apply the ideas to front end development.

She describes the concept of a **_structured event_** - an event &quot;which captures everything that occurred while one particular request interacted with your service&quot; (Chapter 5).

On the front end, a structured event might capture everything that happened during, say, a user interaction or receipt of an HTTP response. This might be done by placing logging calls at key points in the code, similar to how we might place `console.log` statements for local debugging.

In this article I&#39;ll give an overview of front-end observability with some examples using [Sentry](https://sentry.io/).

## Observability on the front end

A key benefit of structured events (compared to unstructured logs) is that they can contain ***rich contextual data points***:

- **User events**, such as what value was entered into an input
- **HTTP events**, such as responses received from a backend
- **Browser events**, such as location data

![Observability points for typical front end applications: HTTP, user and browser events](/images/articles/front-end-observability/observability-points.svg)

These data points are ***queryable*** and we can filter for them using the powerful querying facilities of monitoring platforms such as [Sentry](https://sentry.io/), [DynaTrace](http://dynatrace.com), [AWS CloudWatch RUM](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-RUM-custom-events.html), and [Azure Monitor](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/log-query-overview).

![Querying structured logs in Sentry](/images/articles/front-end-observability/example-stake-error-sentry-query.png)

With structured logs in place, developers can &quot;ask&quot; arbitrary questions about how the application is behaving in production.

We can pro-actively search for unanticipated bugs or diagnose difficult errors involving complex front end logic.


  Capabilities of observable systems
  
  
    In defining &quot;observability&quot; (Chapter 1), Majors lists the following capabilities:
  

  
     - Understand the inner workings of your application
     - Understand any system state your application may have gotten itself into, even new ones you have never seen before and couldn‚Äôt have predicted
     - Understand the inner workings and system state solely by observing and interrogating with external tools
     - Understand the internal state without shipping any new custom code to handle it (because that implies you needed prior knowledge to explain it)
  


Let&#39;s look at some examples.

## Example 1 - User event

![Observability point: user interaction](/images/articles/front-end-observability/observability-points-user-event.svg)

Suppose our front end app validates a change to a numeric field:

When the user edits the stake of a shareholder, we want to validate that the total of all shareholder stakes is never more than 100%. But this only applies to _active_ shareholders ‚Äì we want to skip shareholders which have been turned off.

We want to make the state of our component observable.

Let&#39;s log two structured events:

1. ***changeStakeValid*** - stake was valid; we&#39;ll submit a PATCH
2. ***changeStakeSumError*** - stakes sum was too high; we&#39;ll show an error




  ,
    });

    await patchShareholderStake();

    refreshShareholders();
  } else ,
    });

    showShareholderStakeError();
  }
}
`}
  



Suppose we subsequently receive a bug report:

&gt; The user attempted to enter a 20% stake for one node, but got an error, despite the enabled nodes having a total &lt;= 80%.

We might ask ourselves a bunch of questions:

- Why didn&#39;t our &quot;enabled&quot; logic work in this case?
- Was there a bug in the front end logic?
- If so, how should we fix it?

Instead of having to guess or consult various sources, wouldn&#39;t it be nice if we could more directly observe what happened?

Let&#39;s query Sentry for:

- Events generated by this component - `message:&quot;useShareholderStakeEditor*&quot;`
- For this user - `user.id:1234`
- And this company - `companyId:5678`

![Querying Sentry for structured events by message, user id and company id](/images/articles/front-end-observability/example-stake-error-sentry-query.png)

We find our error event - `changeStakeSumError`:

![Sentry query results indicating an error was logged](/images/articles/front-end-observability/example-stake-sentry-query-results.png)

Clicking on the row reveals some interesting details:

![Sentry error details, showing an item with the tags shareholderId: 113 and stake: 30](/images/articles/front-end-observability/example-stake-sentry-query-result-detail-tags.png)

![Sentry error details, showing an item (id: 112) with the word &quot;[Deleted]&quot; in its name](/images/articles/front-end-observability/example-stake-sentry-query-result-detail.png)

We see that the user was trying to set one shareholder (id: 113) to have a stake of 30%.

However, another shareholder (id: 112), which was `enabled: true`, had the word &quot;Deleted&quot; in its name. The presence of the word &quot;Deleted&quot; likely confused the user into thinking that its 40% stake would not included in the total.

Now we have enough information to propose further actions:

1. Solution: We could add logic that removes words like &quot;Deleted&quot; from names of shareholders, to avoid confusing the user like this in future
2. Solution: We could ask the user to disregard words like &quot;Deleted&quot; and share that knowledge with other users publicly (e.g. via Slack)
3. Investigation: We could find out why users were using the word &quot;Deleted&quot; in shareholder names; is the delete function broken?

Notice that we don&#39;t have to ask the user questions and wait for their response, nor run the application and attempt to reproduce the error, nor query the database, nor puzzle over server-side logs.

Rather, **_we can to go directly to the root of the problem_**, observing logs generated by the specific part of the code which the user was interacting with at the time of the event.

Even better, by naming each logging statement uniquely and using a strict hierarchical naming convention (e.g. `$/$`), we can query more broadly by component, then narrow in on the event, to locate the exact line of code which generated the event!

When you think about it, observability is not all that different from the standard practice of adding `console.log` statements at appropriate points and debugging locally¬†‚Äì only it&#39;s more rigorous and queryable and **_we can observe production_**.

## Example 2 - HTTP event

![Observability point: HTTP event](/images/articles/front-end-observability/observability-points-http-event.svg)

On saving a shareholder&#39;s stake, the front end app should refresh the list of all shareholders&#39; stakes.

But suppose we receive another bug report:

&gt; A user reports that the stakes did not refresh after they saved a stake.

So we ask some questions:

- Why did the refresh not work?
- Was there a problem with the PATCH request?
- Did the front end code handling of the PATCH request fail?
- Was there some other kind of issue?

If we log an event on refresh, it should be easier to find out.



  ) );

        captureEvent(,
          },
        });

        const refetchShareholdersResult = await refetchShareholders();

        captureEvent(,
          },
        });
      } else 
    }
    ...
  }
`}
  


As in the previous example, let&#39;s query Sentry for:

- Events generated by this component - `message:&quot;useShareholderStakeEditor*&quot;`
- For this user - `user.id:1234`
- And this company - `companyId:5678`

We find our two events - `changeStakePatchCompleted` and `changeStakeRefreshCompleted` - were both logged:

![Sentry query results, indicating that both the patch and refresh completed successfully](/images/articles/front-end-observability/example-stake-sentry-update-results.png)

However, examining the refresh event, we see that the new stake value was not provided.

![Sentry refresh details, showing stale value for stake of id: 113](/images/articles/front-end-observability/example-stake-sentry-update-result-detail-refetch.png)

Now we know why the stake did not refresh properly ‚Äì the back end is returning stale data! We&#39;ll need to discuss cache-busting with the back end developers.


## Example 3 - Browser event

![Observability point: browser events](/images/articles/front-end-observability/observability-points-browser-event.svg)

For our final example, suppose the front end app should refresh the list of all shareholders&#39; stakes periodically, e.g. every 30 seconds.

We receive the following bug report:

&gt; A user reports after one user had modified the stakes of certain shareholders, the other users didn&#39;t see the updates until the following day.

We might ask:

- Why did the auto refresh not work?
- Was there a problem with the GET request or code handling the response?
- Was there a Backend issue, such as a cache becoming stale?

Let&#39;s query Sentry for observability on the auto-refresh hook:

- Events generated by the hook - `message:&quot;useShareholdersAutoRefresh*&quot;`
- For this company - `companyId:5678`

We find our autorefresh event - `useShareholdersAutoRefresh` - was logged:

![Sentry query results, indicating that the auto refresh completed successfully](/images/articles/front-end-observability/example-stake-sentry-auto-refresh-results.png)

And hovering over the events column, it looks like it has been running at regular intervals:

![Sentry query results, indicating that the auto refresh ran many times over the last 24 hours](/images/articles/front-end-observability/example-stake-sentry-auto-refresh-results-period.png)

What could have gone wrong? Inspecting the details, we see that different values were returned for one user than for another:

![Sentry event details, indicating a discrepancy between data received for two different users](/images/articles/front-end-observability/example-stake-sentry-auto-refresh-details-compare.png)

This indicates that the back end may be serving stale updates for some users, rather than propagating changes to all users at once.

## Tracing with breadcrumbs

From the previous examples, you may have noticed continuity between events. For example, the user action of editing a shareholder stake generates a sequence of related events: `changeStakeValid`, `changeStakePatchCompleted`, `changeStakeRefreshCompleted`.

Wouldn&#39;t it be nice if we could see a sequence or &quot;flow&quot; of events together in a list?

Thanks to Sentry&#39;s [breadcrumbs feature](https://docs.sentry.io/product/issues/issue-details/breadcrumbs/), we can. Simply open the details of one of the events, scroll down to the Breadcrumbs section, then filter by &quot;Tranaction&quot;.

![Sentry event details, breadcrumbs section, listing a sequence of antecedent events](/images/articles/front-end-observability/example-breadcrumbs-transactions.png)

## Querying across events

With detailed structured events in place, we can form more interesting queries, proactively searching for anomalies or just simply learning more about how our system functions in production.

For example, as we included `companyId` in the events concerned with shareholders, we could more generally query all events associated with that `companyId`. More powerfully, we could query all events associated with **_any_** `companyId`, that is, all company-related events. This could be useful if company was an important entity in our system and we wanted to prioritise fixing of errors related to that entity.

Or take another example ‚Äì querying by date and time. We could query for events with `message:&quot;*error*&quot;`, within a time of day in which users are experiencing a lot of issues. This would allow us to diagnose the cause of those issues separately from more time-independent issues.

## Observability and documentation

Documentation can be an excellent place to surface observability.

Links to queries in a monitoring system can be placed in wiki pages, where they can be discovered by our team members or others in the organisation as needed.

For example, from our previous example, wouldn&#39;t it be great if a new team member could not only read a textual description of Shareholder stakes, but also be linked to actual production data around this feature?

We could achieve this by querying Sentry for all events with `message:&quot;*shareholderStake*&quot;`, grabbing a link to that query and pasting it into a &quot;Shareholder stakes&quot; wiki page, perhaps under a heading titled &quot;Observability&quot; with a link titled &quot;shareholderStake events&quot;.

![Example: Shareholder stake feature documentation with Observability section and link to `shareholderStake` event query](/images/articles/front-end-observability/docs-observability-section.svg)

Imagine if all feature documentation was augmented with links to observability queries. This could give newcomers and experienced team members alike a boost in understanding how each of those features functions in production.

## Managing observability code

In the interests of keeping code clean and readable, we might want to reduce the quantity and complexity of logging code.

Some ideas:

* Abstraction
* Removal
* Aspects

### Abstraction

We can hide logging behind a more abstract function, to reduce its complexity and surface area.

For example, in a React codebase, rather than directly calling `captureEvent` from `@sentry/react`, we could create and consume our own custom hook, named something like `useLogEvent` returning a function like `logEvent`. The hook and function could encapsulate concerns such as caching re-used data and following a hierarchical naming convention.

### Removal

Similar to feature flag controls, logging code could be scheduled for removal after a period of time, if the software has been working well and is considered not in need of monitoring.

Alternately, we could comment-out logging calls or add a special flag to disable them. Developers could quickly determine that the code is not in use and skip over it.

### Aspects

Aspect-oriented programming involves augmenting the behaviour of existing code without modifying it, typically using a declarative pattern such as [decorator](https://www.w3schools.blog/java-decorator-design-pattern). Frameworks such as [AspectJ](https://github.com/eclipse-aspectj/aspectj) are already used for logging in back end systems.

In front end, [Typescript Decorators](https://devblogs.microsoft.com/typescript/announcing-typescript-5-0/#decorators) may in the near future allow logging to be added in a similar, unobtrusive style.

## Security and privacy

A word about security ‚Äì sensitive data (such as [personal data](https://en.wikipedia.org/wiki/Personal_data)) should probably be omitted from structured events, to avoid data leakage and comply with regulations.

The chance of sensitive data being accidentally leaked grows with increased use of logging in production (as with any other use of data production). So if your system deals with sensitive data, it&#39;s crucial to have processes in place to ensure that this data is not leaked in logs. This could be part of a code review process as well as an ongoing independent review process, likely involving examination of both code and logs.

## Conclusion

Observability in software engineering is about observing the internal state of a software system during regular usage in production, typically by capturing and monitoring detailed and structured log outputs.

While more commonly applied to distributed systems on the back end, observability can also be applied to the front end, using front end compatible tools such as Sentry to observe states generated by user, HTTP or browser events.

Distinct from typical logging, structured events capture detailed contextual information surrounding the events and make the events queryable (e.g. with tags in Sentry) and traceable (e.g. with Breadcrumbs in Sentry).

Using structured data generated by production logging, we can diagnose an issue or answer an unanticipated question about the behaviour of the system. Additionally, we can proactively search for issues or anomalies by querying across events. And those queries can be integrated into documentation, where they can be discovered and accessed by engineers or other interested parties.

Observability is a newly emerging field within software engineering, and we can&#39;t know for sure what it will look like in the future. Increasing the observability of a front end could potentially be a very worthwhile pursuit, in terms of time and cost saved, where an application is already running in production and complex issues need to be diagnosed and resolved quickly.

## Code example

The source code for the examples mentioned in this article can be found here:

[https://github.com/jonathanconway/observability-example-react](https://github.com/jonathanconway/observability-example-react)

## Further reading

These resources inspired this article:

- [_Observability Engineering_](https://info.honeycomb.io/observability-engineering-oreilly-book-2022) by Charity Majors
- [_Sentry Browser JavaScript Docs_](https://docs.sentry.io/platforms/javascript) by Sentry
</content>
  </entry>
  

  <entry>
    <title>Refactoring vs Documentation</title>
    <link href="https://conwy.co/articles/refactoring-documentation" />
    <updated>2024-09-13T00:00:00.000Z</updated>
    <id>refactoring-documentation</id>
    <content xml:lang="en" type="html">Most of the software projects I&#39;ve worked on involved complex and poorly structured code. This can happen for various reasons, even with the most dedicated and experienced developers. The problem space is complex, difficult to navigate and/or highly dynamic, time is limited, there is a high turn-over of developers, language and frameworks have limitations.

Refactoring is an oft-touted solution, aiming to bring order to a chaotic code-base by cleaning and improving the code without changing its functionality. Regression testing, via unit tests and end-to-end tests, allow us to verify that the code still performs its intended function.

The problem with refactoring is that almost no team (that I&#39;ve worked on) ever has time for it. There always seem to be other activities that would generate more business value, and faster, and those activities tend to be prioritised. In the professional world, at least, code only exists to achieve an outcome, not as a work of art for its own sake.


  In some situations, such as a rapidly growing or changing business environment, understanding and documenting existing code might be a better use of time than a lengthy refactoring.


Given the above, I&#39;ve come to think that, at least in some situations, such as a rapidly growing or changing business environment, understanding and documenting existing code might be a better use of time than a lengthy refactoring. Bad code can stay bad, but can be worked with effectively, if it is well understood by the development team so that they can work with it efficiently.

If software engineering is the art of managing complexity, software documentation is the art of managing the complexity of software.

Let me dive deeper to explain this perspective.

## Why refactoring takes so long

Code-changes themselves are easier than ever. IDEs provide powerful tools for bulk renaming, find-and-replace, structural search, regular expression search, etc. Unit and integration tests can verify correctness of individual components and groups of components. And practices like inversion-of-control, object-oriented programming and functional programming can support decoupled, flexible code bases that are easy to modify.

The time-consuming part of refactoring is not necessarily the code changes. It can be:

1. Working out what changes need to be made and
2. Ensuring that those changes don&#39;t cause system behaviour errors

During a refactoring (at least, in in any non-trivial code-base) we are likely to learn a lot more about the code, frameworks, business domain than we knew beforehand. As we learn more, our refactoring moves change. This means that what started out as, for example, a simple extraction of a function, can quickly turn into a major alteration, impacting many parts of the codebase.

Additionally, as the scope of the refactoring changes, so too does the scope of the regression testing that will need to be done, to ensure that the changes do not cause breakage. Even with 100% unit test coverage, any code change may open up gaps in coverage, requiring tests to be changed or new tests to be written. The application also needs to be end-to-end tested, whether in an automated or manual manner, and the scope of the end-to-end testing is also impacted.

These factors compound, causing a seemingly small refactoring to turn into a major undertaking, with questionable justification for time spent in proportion to business value.

## Understanding the code

For any significant refactoring to be successful, the Developer likely needs to first have a solid grasp of the code being refactored. This means understanding the structure of the code as well as the expected behaviour of the code and the business problem it is intended to solve. Gaining this understanding takes a significant amount of time, as does making the actual changes. [1](#note-1)

I would argue that such understanding is necessary anyway, not only for refactoring, but for making any changes to the code at all, such as implementing new features, making modifications or fixing bugs.

If we have to spend time understanding code, whether or not we actively perform refactoring, then that understanding itself is of value. So we might as well invest more time into understanding the code, rather than trying to refactor it, given the greater payoff of understanding.

That understanding can be converted into documentation, for future reference and to transfer the knowledge to the rest of the team.

## Documentation structure

A powerful feature of documentation is that can be organised in ways that best facilitate understanding and knowledge transfer.

This is much harder to do in code itself. Code usually has to deal with a mixture of concerns at once, such as the programming language itself, software frameworks, interfacing with other modules and systems (such as databases), security, performance, etc.

![Code usually has to deal with a mixture of concerns](/images/articles/refactoring-documentation/code-mixture-concerns.svg)

For this reason, the way a code base is structured usually does not mirror the structure of the problem it is solving or the feature it is implementing. And even if some of the code could be refactored into a perfect, pure representation of the problem space, the problem space itself may be complex and multi-faceted, making it difficult to represent clearly in code.

Documentation, on the other hand, can be structured in any way or multiple ways at once. So documentation can be divided, grouped and organised in whichever way will best facilitate understanding and communication. For example, documentation can pull together information about each feature in the application into a set of &quot;feature&quot; pages.

![Documentation can describe code while being structured differently, for example, by feature.](/images/articles/refactoring-documentation/documentation-vs-code.svg)

## Documentation as a reference tool

Documentation can serve as a handy reference to consult when a certain question needs to be answered around a particular topic.

For example, suppose a Product Owner asks a Developer about some recent problems encountered with the a &quot;Minimum order free shipping notification&quot; feature. The Developer could consult a feature document which contains links to various resources such as web server logs. The Developer could then follow the link to the web server logs to check if any errors were logged.

So documentation can act as a central repository in which to find pointers to various resources, such as parts of the code and other systems.

![Example: Gathering relevant code, logs, databases into a single feature doc.](/images/articles/refactoring-documentation/documentation-reference.svg)

This referenceability, if used correctly, can make it much easier for a Developer to navigate a complex mass of code modules, databases, services, etc. in order to achieve some goal such as answering a question or diagnosing a bug.

## Flavours of documentation

Let&#39;s look at a few documentation &quot;flavours&quot; that could be applied in various scenarios, depending on the situation.

### Feature documentation

This flavour of documentation focuses on a feature of a software product or system used by customers or other actors.

It may give a brief summary of the feature and also provide some background information such as the business case.

It might then have sub-sections detailing the parts that make up the feature. It might also list the components or systems involved in implementing the feature, including links to code repositories and/or individual code files. It might also contain diagrams depicting user flows, execution flows and/or communication between systems. And it might link to various other flavours of documentation described in this article, such as User interface documentation for the User interface components that make up the feature.

![Example: Outline of minimum order value notification feature documentation](/images/articles/refactoring-documentation/feature-doc-example.svg)

#### How it helps to work with difficult code

- Clarifies how the system behaves, or at least, is intended to behave
- Specifies which code or systems implement the behaviour

### Project documentation

This flavour of documentation is similar to feature documentation, only it focuses on a project (which is time-bound), rather than a feature (which may exist indefinitely).

![Example: Outline of time-limited &quot;Easter discount&quot; project documentation](/images/articles/refactoring-documentation/project-doc-example.svg)

#### How it helps to work with difficult code

- Clarifies why certain code or systems are changing

### User interface documentation

This flavour of documentation describes the various parts of a user interface. A Developer can create this kind of documentation to communicate how the user interface currently works, is intended to work and/or should work in the future.

These docs could be organised as a hierarchy, aligned with the navigation structure of the application&#39;s user interface. Each leaf in the hierarchy has a dedicated page, and that page includes screenshots of that part of the UI, along with descriptive text broken into headings.

![Example: UI documentation hierarchy for a shopping cart](/images/articles/refactoring-documentation/ui-hierarchy-example.svg)

![Example: UI screen documentation](/images/articles/refactoring-documentation/ui-doc-example.svg)

#### How it helps to work with difficult code

- Conceptually maps user interface elements to the code that implements them, when that mapping isn&#39;t made obvious by the code itself
- Clarifies how the user interface currently functions, or at least, is intended to function

### API documentation

This flavour of documentation describes a programming interface of an application, such as the REST API of a web backend.

![Example: API documentation for a PUT Order HTTP endpoint](/images/articles/refactoring-documentation/api-doc-example.svg)

#### How it helps to work with difficult code

- Conceptually maps APIs to the code that consumes them, when that mapping isn&#39;t made obvious by the code itself.
- Clarifies how the APIs currently function, or at least, are intended to function.

### Topic documentation

For material which fits none of the above categories, specific &quot;topic&quot; documents can be created.

Suppose we are trying to describe something which isn&#39;t clearly a feature, a project, a part of the user interface or an API. For example, behaviours of an application which only apply in one particular country, for example, Australia. A topic document titled &quot;Australia&quot; could be created, and grouped under a heading such as &quot;Country-specific behaviours&quot;.

![Example: Topic hierarchy](/images/articles/refactoring-documentation/topic-hierarchy-example.svg)

![Example: Topic documentation](/images/articles/refactoring-documentation/topic-doc-example.svg)

#### How it helps to work with difficult code

- Communicates knowledge on specific topics associated with the code base, which are not clearly expressed by the code itself and don&#39;t fit into other categories of documentation.

## Conclusion

Suitable documentation can facilitate understanding of complex and poorly structured code, enabling developers to work with it more efficiently.

Unlike refactoring, documentation can be added without a full build-deploy cycle, without risking breakage and without having to follow the structure of the code.

Creating documentation may be a better use of time than complex refactoring, if you are dealing with a complex code base, have tight time constraints and need to minimise risk.

## Notes

1 According to research, almost 60% of programmers‚Äô time is spent understanding rather than writing code. See ‚ÄúMeasuring Program Comprehension: A Large-Scale Field Study with Professionals‚Äù by Xin Xia et al. (2017), [https://ieeexplore.ieee.org/abstract/document/7997917](https://ieeexplore.ieee.org/abstract/document/7997917). From the book [_The Programmer&#39;s Brain_](https://www.oreilly.com/library/view/the-programmers-brain/9781617298677/) by Felienne Hermans.

## Further reading

These books inspired this article:

- [_Software Engineering at Google_](https://www.kobo.com/au/en/ebook/software-engineering-at-google) by Titus Winters, Tom Manshreck, Hyrum Wright
- [_The Programmer&#39;s Brain_](https://www.oreilly.com/library/view/the-programmers-brain/9781617298677/) by Felienne Hermans
</content>
  </entry>
  

  <entry>
    <title>Three tests for accessibility</title>
    <link href="https://conwy.co/articles/three-tests-accessibility" />
    <updated>2024-09-13T00:00:00.000Z</updated>
    <id>three-tests-accessibility</id>
    <content xml:lang="en" type="html">There are many good reasons to make our software applications accessible. But to achieve this goal, we must undertake rigorous accessibility testing. 

This presents what may look like an overwhelming challenge: given that there are so many criteria for good accessibility, and that the application itself may be complex in many ways, **how do we verify that all parts of the application are accessible?**

As accessibility is a developing and evolving field, we cannot pretend that there is one silver bullet or one definitive answer. However, I think it&#39;s worthwhile for us to put in a **best effort**.

If we can come up with a small number of tests that cover the most basic and crucial bases of accessibility, then run those tests on all the screens and components of our application, then we can at least say that we have made a significant effort and at most say that we have removed all the most obvious and important impediments to the accessibility of our product.

## Testing on principle

The [WCAG Guidelines](https://www.w3.org/TR/WCAG20/), from which much accessibility advice is derived, are based on [four principles](https://en.wikipedia.org/wiki/Web_Content_Accessibility_Guidelines#WCAG_2.0):

* **Perceivable** - Information and user interface components must be presentable to users in ways they can perceive.

* **Operable** - User interface components and navigation must be operable.

* **Understandable** - Information and the operation of user interface must be understandable.

* **Robust** - Content must be robust enough that it can be interpreted reliably by a wide variety of user agents, including assistive technologies.

I asked one fundamental question of each principle: **what kind of test would verify that this principle had been followed?**

Here are the answers what I came up with:

* **Screen-reader-only**. If I can fully use the application purely by listening to it through a screen-reader, then the application is at least basically &quot;presentable to users in ways they can perceive&quot; and &quot;understandable&quot; through those ways.

* **Keyboard-only**. If I can fully use the application with only a keyboard, then the application is at least basically &quot;operable&quot; by a range of assistive technologies, which operate through the same inputs as the keyboard.

* **Automated test**. If the application passes automated tests, using an appropriate WCAG-compliance testing tool, then it is likely &quot;robust&quot; enough to be be interpreted by various user agents, and meets certain basic technical criteria for being &quot;perceivable&quot; and &quot;operable&quot;.

## Three tests

The three answers lead to three basic tests:

### Test 1: Screen-reader-only

Try to use the application, relying only on **hearing the spoken word**. Turn on a screen-reader and turn off or look away from the screen. You can use the keyboard to provide input as needed.

This tests whether the application is structured in such a way that it can be effectively &quot;presented&quot; to me through one other non-visual assistive technology (a screen-reader). If it can, then it is likely to work almost as well on other non-visual assistive technologies, which rely on the same information that a screen-reader relies on.

Tools:
* [VoiceOver](https://help.apple.com/voiceover/mac/10.14/) (built-in to MacOS and iOS)
* [TalkBack](https://support.google.com/accessibility/android/answer/6283677?hl=en-GB) (built-in to Android)
* [Narrator](https://support.microsoft.com/en-us/help/22798/windows-10-complete-guide-to-narrator) (built-in to Windows 10+)
* [NVDA](https://www.nvaccess.org/) (other versions of Windows)
* [ChromeVox](https://www.chromevox.com/) (Chrome browser on all operating systems)

### Test 2: Keyboard-only

Try to use the application, relying only on **keyboard input**. Put the mouse away or disconnect it, or disable your trackpad.

This tests whether the application is &quot;operable&quot; by a range of assistive technologies, which operate similarly to a keyboard. For example, speech recognition facilities or braille keyboards, which interpret signals analogously to how a keyboard interprets certain keystrokes.

Tools:
* Just your keyboard!

### Test 3: Automated test

Run an automated testing tool on your application, analyse the output and address all major errors detected.

For everything that cannot be captured by tests 1 and 2, automated testing tools can provide some coverage. Of course, an automated tool is just a piece of software and cannot replace aware, focussed human attention. However, it can catch obvious errors that a human may miss, due to human error. It can also thoroughly cover many areas in a short space of time, where a human would take much longer.

Tools:
* [WAVE](https://wave.webaim.org/) by WebAIM (all major operating systems). This tool analyses any web page and provides a detailed report, covering the entire WCAG specification, and highlighting errors.

## Benefits of manual testing

You&#39;ll notice that two out of the three tests are entirely manual and don&#39;t rely on automated tools. While manual testing is harder than just running an automated tool, I think it offers two key advantages:

### 1. It uncovers errors that no automated tool can capture

By actually trying to use our interface, we get a rich, qualitative answer to the question: &quot;how usable is this?&quot;. We can directly observe when the interface is difficult, cumbersome, unclear, or otherwise unusable. We can also directly observe when the interface works smoothly and is easy to use.

A web page might have perfectly structured content, proper usage of semantic HTML and alternative text on all non-textual content. **But what if a user has to listen through 3 minutes of audio, just to sign up for an email alert?**

This is just one example of errors in the interaction design and/or code, which are generally not picked up by automated testing tools.

By actually using an application the way a user would, we can directly identify issues that aren&#39;t clear-cut enough for an automated tool to detect.

Of course, manual testing the application ourselves won&#39;t give us as much information as observing other people try to use it. However, it will probably reveal the biggest and most obvious accessibility issues, giving us an opportunity to resolve them sooner.

### 2. It puts us in the shoes our users

Manual testing encourages us to empathise with our users. This mindset of empathy is a crucial component of good usability, as it affects how we build, what we build and what we prioritise.

## Play well with assistive technologies

One lesson I learned from observing a wide range of users during usability testing was that **users rely a lot on assistive technology, independent of particular applications**.

Many accessibility affordances, from navigating a form to interacting with navigation, are already built in to screen readers and input devices, which are constantly improving and innovating.

* Screen-readers get better at interpreting interfaces and text.
* Input devices are improved to offer more precise and easy-to-use affordances; new input devices come on the market.
* Browsers and operating systems are improve the integration of accessibility features into the user experience.

Rather than trying to anticipate and implement every conceivable accessibility feature directly into our applications, we should instead **focus on making sure our application plays well with assistive technologies**.

We should simply expose the right structures and data and let assistive technologies take it from there. For example, in a rich web application, this means using properly marked-up form elements to label fields and capture form inputs.

![Photo of a person putting their finger on a braille reading device](https://upload.wikimedia.org/wikipedia/commons/4/4a/Plage-braille.jpg)

![Photo of a person using a mouth-held stylus to operate a screen](https://i.pinimg.com/originals/8f/11/23/8f11237b7a530bdfca68f34c5c051952.jpg)

## Conclusion

Rather than getting overwhelmed and giving up on accessibility, might we serve our users better by spending some time on basic testing and letting assistive technologies do most of the heavy lifting? I think the answer is yes!

By means of simple but thorough testing, and making fixes as needed, we will be well our way to making accessible products that work for all of our users.

## Further reading

Boooks that inspired me:

- [_Engineering for Accessibility_](https://www.microsoft.com/en-au/download/details.aspx?id=19262) ‚Ä¢ Jason GRIEVES, Masahiko KANEKO
</content>
  </entry>
  

  <entry>
    <title>Combinatorial Testing</title>
    <link href="https://conwy.co/articles/combinatorial-testing" />
    <updated>2024-09-13T00:00:00.000Z</updated>
    <id>combinatorial-testing</id>
    <content xml:lang="en" type="html">Have you recently tried to unit test a function which has many combinations of possible inputs and expected outputs?

An increasingly common way of writing such a test is to utilize a data-driven test. The problem with data-driven tests is that they can quickly grow to be large and unwieldy.

In this article, I want to introduce a technique for generating data-driven tests without having to spell out every individual combination of inputs/outputs in code.

But first, a quick refresher on data-driven tests...

## Limitations of data-driven tests

You may be familiar with [data-driven testing](https://en.wikipedia.org/wiki/Data-driven_testing). Basically you write a table of combinations of inputs and outputs in which each test case is inputted as &quot;row&quot; of data. Data-driven testing is now supported by many popular test frameworks (Jest, JUnit, NUnit to name a few).

One problem with data-driven testing is: sometimes we have so many combinations to cover that a comprehensive data-driven test would be very lengthy and difficult to read or maintain.

Imagine, for example, trying to write a data-driven unit test for a function which returns the **number of days in a given month**.

The function takes two parameters: `year` and `month` and returns one `day` value. It has to deal with a range of values for each parameter. Multiplying all values that need to be tested by two parameters yields a large number of combinations.

```ts
it.each([
  ,
  ,
  ,
  ,

  // and so on and so on ... üòì
]);
```

That number of combinations, though easy for a computer to process, is not easy for us to wrap our human minds around!

Perhaps the solution is to express the combinations in a more concise manner ‚Äì as grouped ranges of values ‚Äì rather than spelling out every single combination.

Combinator function to the rescue!

## What is a combinator?

In the world of functional programming, the term &quot;combinator&quot; informally refers to a pattern...

&gt; &quot;where complex structures are built by defining a small set of very simple &#39;primitives&#39;, and a set of &#39;combinators&#39; for combining them into more complicated structures&quot;
&gt; ‚Äì _Combinator Pattern_ ‚Ä¢ [wiki.haskell.org/Combinator_pattern](https://wiki.haskell.org/Combinator_pattern)

The combinator I present in this article is more specific. It takes as input an object whose properties each have a value that is an array. Then it combines each value of each array. All of the objects generated by this means are then returned to the caller.

For example, suppose we provide an input object having a single property, &quot;color&quot;, whose value is an array containing elements &quot;red&quot; and &quot;blue&quot;:

```ts

```

The combinator will return us an array having the following objects:

1. an object having a &quot;color&quot; property whose value is &quot;red&quot; and
2. an object having a &quot;color&quot; property whose value is &quot;blue&quot;

```ts
[
  ,
  ,
];
```

Suppose we provide an additional property in our input object, &quot;brightness&quot;, whose value is an array containing elements 100 and 200:

```ts

```

The combinator will return us an array having every specified combination of &quot;color&quot; and &quot;brightness&quot;:

1. an object having a &quot;color&quot; property whose value is &quot;red&quot; and a property &quot;brightness&quot; whose value is 100 and
2. an object having a &quot;color&quot; property whose value is &quot;red&quot; and a property &quot;brightness&quot; whose value is 200 and
3. an object having a &quot;color&quot; property whose value is &quot;blue&quot; and a property &quot;brightness&quot; whose value is 100 and
4. an object having a &quot;color&quot; property whose value is &quot;blue&quot; and a property &quot;brightness&quot; whose value is 200

Like this:

```ts
[
  ,
  ,
  ,
  ,
];
```

Providing a definition object as input, we can get a large set of results as output.

Here&#39;s a high-level diagram:

![UML diagram depicting combinatorial test definition and results](/images/articles/combinatorial-testing/mermaid/mermaid-diagram-2024-08-12-135445.svg)


  combinate ..&gt; Definition~T~

  class combinate~T~ 
  combinate ..&gt; Array~T~ : returns
  Array~T~ ..&gt; T

  class Array~T~ 

  class T 
`} */}

Let&#39;s apply this combinator to a slightly more &quot;real world&quot; example.

## An example: days in a month

For historical reasons, determining the number of days in a month in the Western calendar is complicated.

The following short rhyme tries to summarize the rules in a memorable way:

&gt; Thirty days have September,
&gt;
&gt; April, June and November.
&gt;
&gt; All the rest have thirty-one,
&gt;
&gt; except February alone, which has
&gt;
&gt; twenty-eight days each year
&gt;
&gt; and twenty-nine days each leap-year

Suppose we wanted to unit-test a function, `getDaysInMonth`, which takes `month` and `year` as input and returns a number of `days`.

We could simply input every possible date into the unit test and assert on the month of each. As mentioned above, that could involve quite a lot of fiddling in Excel and would result in a very long and not very human-readable test file.

Instead, let&#39;s try to tackle this problem with a combinator.

Starting with the first two lines of the rhyme:

&gt; Thirty days have September,
&gt;
&gt; April, June and November.

We can express this &quot;thirty days&quot; combination set programmatically, like this:

```ts
const thirtyDays = combinate();
```

The result can easily be passed into a data-driven test in Jest:

```ts
it.each(thirtyDays)(
  &quot;$month in $year should have $expectedDays days&quot;,
  () =&gt; 
);
```

On running the unit test, the following test cases will be generated and executed:

```ts
‚úì april in 2020 should have 30 days (3 ms)
‚úì june in 2020 should have 30 days
‚úì september in 2020 should have 30 days
‚úì november in 2020 should have 30 days
‚úì april in 2021 should have 30 days
‚úì june in 2021 should have 30 days (1 ms)
‚úì september in 2021 should have 30 days
‚úì november in 2021 should have 30 days (1 ms)
‚úì april in 2022 should have 30 days
‚úì june in 2022 should have 30 days
‚úì september in 2022 should have 30 days
‚úì november in 2022 should have 30 days
‚úì april in 2023 should have 30 days
‚úì june in 2023 should have 30 days
‚úì september in 2023 should have 30 days
‚úì november in 2023 should have 30 days
```

Notice how we can use a small amount of code (in this example, 5 lines for the `combinate` call) to generate a much larger set of test cases (16). This gives our test code more leverage.

Covering the remaining lines of the rhyme:

&gt; All the rest have thirty-one,

```ts
const thirtyOneDays = combinate();
```

The following data will be generated:

```ts
‚úì january in 2020 should have 31 days (2 ms)
‚úì march in 2020 should have 31 days (1 ms)
‚úì may in 2020 should have 31 days (1 ms)
‚úì july in 2020 should have 31 days (1 ms)
‚úì august in 2020 should have 31 days
‚úì october in 2020 should have 31 days
... etc ...
```

&gt; except February alone, which has
&gt; twenty-eight days each year

```ts
const februaryDays = combinate();
```

```ts
‚úì february in 2023 should have 28 days (2 ms)
```

&gt; and twenty-nine days each leap-year

```ts
const februaryLeapYearDays = combinate();
```

```ts
‚úì february in 2024 should have 29 days (2 ms)
```

Finally, putting it all together, here is the complete unit test:

```ts
describe(&quot;getDaysInMonth&quot;, () =&gt; );

  const thirtyOneDays = combinate();

  const twentyEightDays = combinate();

  const twentyNineDays = combinate();

  it.each([
    ...thirtyDays,
    ...thirtyOneDays,
    ...twentyEightDays,
    ...twentyNineDays,
  ])(
    &quot;$month in $year should have $expectedDays days&quot;,
    () =&gt; 
  );
});
```

Notice that we can assign meaningful names to each of the variables, increasing the readability of the test code.

I&#39;m sure you would agree that this test code, using a combinator, is more concise and readable than a large table of numbers and strings!

In closing, I encourage you to use combinatorial testing to shorten and sweeten your data-driven tests, thus testing your software thoroughly and making it maximally robust.

## Introducing combinator-util

If you&#39;d like to add a little combinatorial goodness to our unit tests, please check out this re-usable, open-source NPM package:

[https://github.com/jonathanconway/combinator](https://github.com/jonathanconway/combinator)

Contributions welcome!

## Further reading

These books inspired this article:

- [_Introduction to Mathematical Thinking_](https://www.amazon.com/Introduction-Mathematical-Thinking-Keith-Devlin) by Keith Devlin
</content>
  </entry>
  

  <entry>
    <title>Testing Steps</title>
    <link href="https://conwy.co/articles/testing-steps" />
    <updated>2024-09-13T00:00:00.000Z</updated>
    <id>testing-steps</id>
    <content xml:lang="en" type="html">## Introduction

Over the years several [acceptance testing frameworks](https://en.wikipedia.org/wiki/Acceptance_testing#List_of_acceptance-testing_frameworks) have risen and fallen in popularity.

We have seen [Behavior Driven Development (BDD)](https://en.wikipedia.org/wiki/Behavior-driven_development), implemented in formats such as [Cucumber](https://en.wikipedia.org/wiki/Cucumber_(software)) and [RSpec](https://en.wikipedia.org/wiki/RSpec), aiming to provide a human-readable and machine-parsable syntax for defining requirements. Developers and business people collaborate to produce specs in a &quot;Given/When/Then&quot; structure. Developers then implement an executable test for each spec using a more standard programming language test framework, such as JUnit, XUnit, Jasmine, etc.

Though it has grown in popularity, the BDD style has two disadvantages:

1. The &quot;Given/When/Then&quot; syntax adds an additional learning curve for people not familiar with the language.
2. The &quot;Given/When/Then&quot; structure (if adhered to) constrains the tests, forcing all assertions to take place after all actions, rather than allowing a sequence of interleaved actions and assertions.

Instead of BDD, we could use a much simpler and more familiar syntax:

**An ordered list of testing steps**

In this _testing steps_ approach, we remove the Given/When/Then structure altogether and simply list our sequence of steps and assertions. As with BDD tests, we then write code that implements each of the steps, using templating and parameterization where appropriate for reusability.

Instead of:

```
GIVEN x
WHEN y
THEN z
```

We write this:

```
1. x
2. y
3. z
```

## An example

Suppose we wish to write a spec for the following requirement:

&gt; Display an error if a currency conversion is over the limit for that currency, along with a Max button which resets the payment amount to the maximum amount, and then allows the user to proceed with the payment at that amount.

This requirement might be captured in two BDD specs such as the following:

&gt; ### TITLE: Validate currency limit with max button
&gt;
&gt; ### SCENARIO 1: Validate currency limit
&gt;
&gt; - **GIVEN** I am a registered user
&gt; - **AND** I have a bank balance of 100,000 GBP
&gt; - **AND** The maximum conversion from GBP to CAD is 50,000
&gt; - **WHEN** I go to the Make a Payment screen
&gt; - **AND** I set the Destination currency to CAD
&gt; - **AND** I set the Payment amount to 51,000 GBP
&gt; - **THEN** I will see a Currency conversion over daily payment limit error
&gt; - **AND** I will see a Fill max currency button
&gt;
&gt; ### SCENARIO 2: Provide Max button, which resets currency to limit value
&gt;
&gt; - **GIVEN** I am a registered user
&gt; - **AND** I have a bank balance of 100,000 GBP
&gt; - **AND** The maximum conversion from GBP to CAD is 50,000
&gt; - **WHEN** I go to the Make a Payment screen
&gt; - **AND** I set the Destination currency to CAD
&gt; - **AND** I set the Payment amount to 51,000 GBP
&gt; - **AND** I click the Fill max currency button
&gt; - **AND** I click the Submit payment button
&gt; - **THEN** I will see a Payment successful screen
&gt; - **AND** I will see the amount paid as 50,000 GBP

Notice how cumbersome and repetitive this is.

Using a ***testing steps*** format, we could replace it with a single, neatly condensed sequence of steps:

&gt; ### SCENARIO: Validate currency limit with max button
&gt;
&gt; 1. Log in as a registered user
&gt; 2. Assume a bank balance of 100,000 GBP
&gt; 3. Assume a maximum conversion from GBP to CAD of 50,000
&gt; 4. Go to the Make a Payment screen
&gt; 5. Set the Destination currency to CAD
&gt; 6. Set the Payment amount to 51,000 GBP
&gt; 7. Observe the following error is visible: Currency conversion over daily payment limit
&gt; 8. Observe the following button is visible: Fill max currency button
&gt; 9. Click the Fill max currency button
&gt; 10. Click the Submit payment button
&gt; 11. Observe the following success message: Payment successful screen
&gt; 12. Observe the following field | value: Amount paid | 50,000 GBP

Notice how this latter form conveys the same information as the BDD spec, but without the Given/When/Then structure, and as a sequence of actions/events in a single flow.

Also notice that this is closer to how most human beings would manually test this kind of behavior. They wouldn&#39;t separate their testing into two sets of three distinct phases, starting over again after the first set. Rather, they would more likely perform just one sequence of steps, verifying the correctness as they go, all the way until the last step.

It&#39;s true that the testing steps don&#39;t explicitly tell us which of the steps are arrangements/pre-conditions, which are actions and which are assertions/post-conditions. For example, step 8 doesn&#39;t explicitly tell us that it is an assertion. However, I would argue that this fact is implicit in the language anyway and the average reader should have no problem interpreting a statement like &quot;I will see a Fill max currency button&quot; as an expectation rather than an action for the reader to perform.

From the developer&#39;s point of view, it doesn&#39;t matter either; any of these steps can have its own code block, associated via string/template matching. We don&#39;t need to specify whether a step is a Given, a When or a Then, in order to match the step to the correct code block. (If we want to make that attribute explicit in code, we can always do so with a comment, decorator, method naming convention, etc.)

## Conclusion

It seems to me that the &quot;Given/When/Then&quot; way of structuring spec tests is a relic of design by contract and intended to help the code more than the user. It is unnecessary to structure tests in this way. Instead we can use a simple sequential list of steps. This is simpler, more user-friendly and more suitable to typical testing in which actions and assertions are intermingled throughout a sequence.

Users don&#39;t normally think in terms of pre-conditions/post-conditions, but are much more likely to think in terms of sequence of actions they perform and responses they get from the system.

## Library

During writing of this article I developed a new testing framework which applies the concept of testing steps.

You can check it out here: `testing-steps`.

This framework is a Javascript/Typescript library which can be consumed by unit tests targeting the Jest test runner.

If there is enough interest, I will look at getting it ported to other languages/frameworks.

## Further reading

Books that inspired me:

- [_The Cucumber Book_](https://pragprog.com/titles/hwcuc2/the-cucumber-book-second-edition/) ‚Ä¢ Matt WYNNE
</content>
  </entry>
  

  <entry>
    <title>Diagramming Typescript</title>
    <link href="https://conwy.co/articles/diagramming-ts" />
    <updated>2024-09-13T00:00:00.000Z</updated>
    <id>diagramming-ts</id>
    <content xml:lang="en" type="html">As the practice of front-end development grows and evolves, front-end developers find themselves working with increasingly complex problem domains, requirements and code-bases. This necessitates increasing usage of solution design and planning tools, such as [wireframes](https://en.wikipedia.org/wiki/Website_wireframe), [component diagrams](https://reactjs.org/docs/thinking-in-react.html#step-1-break-the-ui-into-a-component-hierarchy), [user stories](https://www.martinfowler.com/bliki/UserStory.html), etc.

In recent work on large, complex Typescript code-bases I&#39;ve found myself doing a significant amount of upfront solution design, planning modifications or additions to the code by way of high-level diagrams.

In this article I want to share a &#39;tweaked&#39; form of UML that I&#39;ve been using in solution designs on Typescript code-bases.

But let&#39;s first review the utility of diagrams and UML.

## Why diagrams?

To motivate this article, I want to review uses and benefits of diagrams.

Diagrams have the following features, distinct from either code or documentation:

- ***Partial.*** Diagrams can &quot;sketch&quot; some parts of code while omitting others
- ***High-level.*** Diagrams can depict high-level components while omitting low-level implementation details
- ***Spatial.*** Diagrams are in 2D space, enabling us to better visualise the parts and how they connect to eachother

By virtue of these features, diagrams offer certain unique uses and benefits at different stages of the software lifecycle.

- Solution design
- Planning complex changes
- Documentation

### Solution design

Solution design helps us to conceive our solutions before implementing them. We can begin to assemble the pieces of the solution and envisage how they will interact with eachother in advance of writing any code. This helps us to clarify our work, avoid costly mistakes and rework and come up with better time estimates. The benefits are multiplied when solution design is shared between multiple team members and improved based on their feedback.

Diagramming is an excellent way to both develop and communicate a solution design. Diagrams, which don&#39;t need to be compiled, can be built partially, creating a kind of &quot;code sketch&quot; that communicates software design at a high level while omitting details. Diagrams are laid out spatially, allowing us to organise our thinking outside the constraints of the file system or compiler.


  Multi-stakeholder solution design
 
  In some organisations multiple stakeholders may need to approve a solution ‚Äì from product owners to designers to security specialists. In these cases it&#39;s even more beneficial to develop solution designs and share them with the stakeholders. The stakeholders can then have an opportunity to identify issues and risks before implementation commences. They can co-design the solution with developers, shaping it in beneficial ways before implementation begins.



  Agile solution design

  There is a culture in some development teams of avoiding solution design, thinking it is unnecessary and even to be avoided, since it is part of undesirable &quot;big upfront planning&quot; or &quot;waterfall&quot; methodology. The idea is that &quot;agile&quot; is a new and better way of working and, as such, developers should begin coding as soon as possible with minimal planning and preparation (&quot;MVP&quot;).
  
  My thinking goes against this, but addressing it in-depth is beyond the scope of this article. But to summarise: if you carefully review the agile literature, it is very rare for it to discourage planning, big picture thinking, solution design or architecture.
  
  In fact, some of the most popular agile practitioners, such as Bob Martin and Martin Fowler, have written lengthy books on both architecture and UML. Thus it is apparent that agile is broadly compatible with solution design and planning.


### Planning complex changes

As our software grows in multiple directions (code size and complexity, users, features, etc.) the complexity of making changes increases. Any change, from renaming a field to adding cross-cutting functionality such as monitoring, can involve changes to many components across the code-base. We may need to envisage the impacts of these changes and carefully plan them out, paying regard to constraints such as time, system resources, performance, etc.

This is where diagramming can come in handy. As we analyse the code directly, we can also begin to sketch out a partial model of the code, focussing on just the components that will need to be changed. We can then add our changes to this model and use annotations, dashed rectangles or whitespace to mark our which parts are changing.

### Documentation

Diagrams can form a useful part of documentation. They can be added to wiki pages, task trackers and pull requests, to help developers and other team members understand the structure of the code.

We can create a diagram only depicts once slice of the code-base, and omit implementation detail. This helps when documenting a cross-cutting aspect of the code-base.

For example, we might have one page in the wiki dedicated to the topic of &#39;SMS verification&#39;. That page could detail the systems and processes involved in sending an SMS verification message. It could include a diagram depicting only the parts of our code involved in SMS verification and how they connect to eachother, omitting parts that aren&#39;t directly involved in SMS verification.

## Background on UML

UML is a widely used diagramming language for depicting object-oriented code structures. Normally used with strongly-typed, class-based programming languages, such as Java and C#, it focuses on representing the public interfaces of classes, interfaces and other structures, inheritance relationships between classes (generalisation, realisation, etc.) and relationships between objects generated by classes (association, aggregation, composition).

The UML standard covers a range of diagrams, most of which can be readily applied to Typescript and/or front-end projects with practically no tweaking.

Notice, however, that UML &quot;class diagrams&quot; (as the name suggests) are focussed on depicting classes and objects. Most Typescript code-bases (especially front-end) are much more focussed on functions and types. This is in good part due to the functional style of programming that predominates in front-end languages (Javascript), frameworks (React), libraries (Redux) and tooling.

This presents a challenge: there is a dissonance between the class focussed world of UML and the function and type focussed world of Typescript. It seems like we need to tweak UML in order to use it effectively in a Typescript context.

Happily, as we will see, this is all quite doable. In fact, &quot;Typescript UML&quot; can be realised as a subset of UML without significantly altering the language.

## Applying UML to Typescript

The first thing to note is that with Typescript we need to model functions and types ‚Äì structures which aren&#39;t traditionally supported by UML.

UML already allows for extensibility, via the &quot;stereotype&quot; pattern, annotations and on connector lines. We can carefully apply use these features to depict important Typescript structures such as types and functions, while preserving the overall idioms of UML, to keep the diagrams clean, consistent and (if needed) broadly accessible to non-Typescript developers.

## Interfaces, types and enums

Interfaces and enums, which are also present in class-based languages, translate immediately over to UML.



  class BaseUser 
  BaseUser ..&gt; UserType
*/}

![UML diagram depicting an interface, a type and an enum](/images/articles/diagramming-ts/interfaces-types-enums.svg)

```typescript
enum UserType 

interface BaseUser 
```

Types can get a little more tricky. A type might simply be declared equivalent to another structure ‚Äì such as an interface or another type. Or it might also be a composition of other structures, such as a conditional type or mapped type.

How can we accurately represent our types diagrammatically without overburdening our diagrams with code-like detail?

My approach here is to simply lay out all the types involved and depict their relationships to eachother without necessarily including logical constraints or finer-grained details such as mapped properties. Where such details are crucial, they can be placed in a nearby &#39;note&#39; element (already a feature of standard UML) and/or, as in the case of mapped properties, simply included in the type&#39;s first compartment.



  class BaseUser 
  BaseUser ..&gt; UserType

  class Pilot 
  Pilot --|&gt; BaseUser

  class Crew 
  Crew --|&gt; BaseUser

  class User 
  User --|&gt; Pilot
  User --|&gt; Crew

  class UserAccountInfo 
  UserAccountInfo --|&gt; BaseUser: (pick)
*/}

![UML diagram depicting a group of related types](/images/articles/diagramming-ts/interfaces-types-enums.svg)

```typescript
type PilotLicenceNumber = string;

type Pilot = BaseUser &amp; ;

type Crew = BaseUser &amp; ;

type User = Pilot | Crew;

type UserAccountInfo = Pick;
```

I&#39;m not sure if this is ideal, but it seems a reasonably pragmatic approach. Note that UML allows us to depict the code partially, not necessarily exhaustively.

We can also depict ***associations*** between different interfaces/types in the same way as regular UML class diagrams. In this example, we depict a `Flight` interface which aggregates `Crew` and `Pilot` members, along with the cardinality of the relationship.



  class Crew 

  class Flight 
  Flight &quot;0&quot; o--&gt; &quot;n&quot; Crew : crews
  Flight &quot;0&quot; o--&gt; &quot;n&quot; Pilot : pilots
*/}

![UML diagram depicting types with their associations](/images/articles/diagramming-ts/interfaces-types-associations.svg)

```typescript
interface Flight 
```



## Functions

As the name suggests, UML &quot;class diagrams&quot; are normally oriented toward depicting classes, which are treated as the main building blocks of class-based programs.

Typescript programs however, especially on the front-end, tend to more heavily emphasise functions. Functions are treated as &quot;first class citizens&quot;, meaning that they make up important structural elements of the program, and are not merely an implementation detail.

Nevertheless, we can take UML&#39;s &quot;box with two compartments and a title bar&quot; and re-purpose it for diagramming Typescript functions.

The public interface of a Typescript function primarily consists of its parameters and return type. We can repurpose the first compartment of our box to depict the parameters passed in to the function. Since a function has no publicly accessible &quot;instance&quot; members, there&#39;s no need to represent them at all. The lower compartment can contain private variables held in scope of the function, which, as with private members of a class, aren&#39;t accessible from outside.


  isValidUser ..&gt; User : user
  isValidUser ..&gt; isValidPilotLicenceNumber : (calls)

  class isValidPilotLicenceNumber 
  isValidPilotLicenceNumber ..&gt; PilotLicenceNumber : licenceNumber
*/}

![UML diagram depicting functions](/images/articles/diagramming-ts/functions.svg)

```typescript
function isValidUser(user: User): boolean 

function isValidPilotLicenceNumber(licenceNumber: PilotLicenceNumber): boolean 
```

This leaves one important problem ‚Äì how do we represent the return type of a function?

## Return types of functions

Since a Typescript function only has one return type, we might want to represent it as one structure. That type could have one or more members (if it is an inline type, interface or class). It could also have relationships to other types (e.g. an interface that realises another interface). It could even be another function.

We could designate an additional, third, compartment in which to place information about the return type. There are two downsides to this, however. Firstly, introducing a third compartment increases the learning curve for someone who is more accustomed to seeing only two compartments in a UML box diagram. They must figure out what the third compartment signifies and then remember that it signifies the return type and that they should look there for the return type. Secondly, there is the awkward problem of representing a return type which isn&#39;t simply a collection of members. How do we represent a return type that itself has a relationship with another type? Or a return type that is itself a function? If we simply list a single name in the third compartment as though it is a member, this creates confusion as to whether we are naming the return type itself or a member of the return type. For the above reasons it seems inconvenient to house our return type in the third compartment ‚Äì or any compartment ‚Äì of a Function box.

A better way is to put the return type in a separate box altogether. We can actually do this, in much the same way as we would represent a type of a function parameter. The relationship can easily be clarified with a connector, which points from the function box to the return type box with a &#39;returns&#39; label.


  fetchFlightDetails ..&gt; Flight : (returns)


  class Flight 
*/}

![UML diagram depicting a function and its return type](/images/articles/diagramming-ts/function-return-type.svg)

```typescript
async function fetchFlightDetails(id: string): Flight 
```



## Framework-specific functions

We can do a similar re-purposing to support framework-specific building blocks which are functions ‚Äì for example, React **components** and custom **hooks**.

UML includes a &quot;stereotype&quot; pattern ‚Äì a double-angle-bracketed name that sits above the title. This can be used to label our functions ‚Äì e.g. `&lt;&gt;`, `&lt;&gt;` for React-specific functions. These, along with the aforementioned ways of depicting functions and types, can be used to diagram the components of a React application.


  useFlightDetails ..&gt; Flight : (returns)
  useFlightDetails ..&gt; fetchFlightDetails : (calls)

  class FlightDetails 
  FlightDetails ..&gt; useFlightDetails : (calls)
  FlightDetails &quot;1&quot; *--&gt; &quot;1..m&quot; PilotDetails : (renders)
  FlightDetails &quot;1&quot; *--&gt; &quot;1..m&quot; CrewDetails : (renders)

  class PilotDetails 
  PilotDetails ..&gt; Pilot

  class CrewDetails 
  CrewDetails ..&gt; Crew
*/}

![UML diagram depicting React components and hooks](/images/articles/diagramming-ts/react-components-hooks.svg)

```typescript
function useFlightDetails(: ):  

function FlightDetails(: ): React.Node 

function PilotDetails(: ): React.Node 

function CrewDetails(: ): React.Node 
```

Note: As React components typically take a single &#39;props&#39; object as a parameter, I opted to just inline that object&#39;s members in the first compartment of the `&lt;&gt;` box. This very small inconsistency probably won&#39;t be too confusing to anyone who has a basic understanding of React.



## Putting it all together

For your reference, here is one big UML diagram comprising all the pieces discussed in this article:



  class BaseUser 
  BaseUser ..&gt; UserType


  class string 

  class PilotLicenceNumber 
  PilotLicenceNumber --|&gt; string

  class Pilot 
  Pilot --|&gt; BaseUser
  Pilot ..&gt; PilotLicenceNumber : licenceNumber
  Pilot ..&gt; UserType : type

  class Crew 
  Crew --|&gt; BaseUser
  Crew ..&gt; UserType : type

  class User 
  User --|&gt; Pilot
  User --|&gt; Crew

  class Flight 
  Flight &quot;0&quot; o--&gt; &quot;n&quot; Crew : crews
  Flight &quot;0&quot; o--&gt; &quot;n&quot; Pilot : pilots


  class isValidUser 
  isValidUser ..&gt; User
  isValidUser ..&gt; isValidPilotLicenceNumber : (calls)

  class isValidPilotLicenceNumber 
  isValidPilotLicenceNumber ..&gt; PilotLicenceNumber : licenceNumber



  class fetchFlightDetails 
  fetchFlightDetails ..&gt; Flight : (returns)




  class useFlightDetails 
  useFlightDetails ..&gt; Flight : (returns)
  useFlightDetails ..&gt; fetchFlightDetails : (calls)

  class FlightDetails 
  FlightDetails ..&gt; useFlightDetails : (calls)
  FlightDetails &quot;1&quot; *--&gt; &quot;1..m&quot; PilotDetails : (renders)
  FlightDetails &quot;1&quot; *--&gt; &quot;1..m&quot; CrewDetails : (renders)

  class PilotDetails 
  PilotDetails ..&gt; Pilot

  class CrewDetails 
  CrewDetails ..&gt; Crew
*/}

![UML diagram depicting all the ideas discussed in this article](/images/articles/diagramming-ts/all-together.svg)

With all these parts in one diagram, including connective lines, we can perhaps see more clearly one of the main benefits of diagramming: being able to zoom out and see how all the parts connect together to form the whole.

We can, for example, easily see which components depend on the core types `Pilot` and `Crew`. During initial solution design, this diagram might help us to estimate and prioritise the work. Or during a complex change, it might help to visualise the impact, were we to modify one or both of these types.

This kind of &quot;birds-eye view&quot; wouldn&#39;t be possible with just code alone, which appears in a hierarchy of folders and files. Even if we expanded every folder, we still wouldn&#39;t see all the connections between the structures contained in the files. Diagrams give us a more powerful visualisation of our code.

## Future directions

Many UML-code and code-UML converters already exist, supporting class-based programming languages such as Java and C#. It would be great to see such tools implemented for Typescript. [tplant](https://github.com/bafolts/tplant) looks like a promising start, though it appears to only support the code-UML direction.

It would be interesting to see if subsets of UML emerge, focussed on representing functional and/or Javascript/Typescript structures.

State-charts have already been recommended for diagramming Redux state machines. Perhaps it would be better for developers to standardise on UML state diagram notation.

## Further reading

These books may serve as a handy guide and reference on UML:

- [_UML Distilled_](https://www.martinfowler.com/books/uml.html) ‚Ä¢ Martin FOWLER
- [_The Unified Modelling Language User Guide_](https://www.amazon.com/Unified-Modeling-Language-User-Guide/dp/0321267974) ‚Ä¢ Grady BOOCH
- [_Modelling with UML - Language, Concepts, Methods_](https://www.abbeys.com.au/book/modeling-with-uml-language-concepts-methods-book-9783319816357.do) ‚Ä¢ Bernhard RUMPE
</content>
  </entry>
  

  <entry>
    <title>Uses of mock data</title>
    <link href="https://conwy.co/articles/mock-data" />
    <updated>2024-09-13T00:00:00.000Z</updated>
    <id>mock-data</id>
    <content xml:lang="en" type="html">Mock data, dummy data, fake data, test data, sample data.

These terms all express the same thing:

**Data that a developer hard-codes in place of real data.**

In my observation, mock data has tended to be used in a rather loose, slipshod, careless manner. Unlike documentation, it is treated as the garbage of software material. (Sometimes even referred to as &quot;garbage data&quot;). People will try to avoid writing it by using elaborate &quot;generators&quot; such as [jFairy](https://github.com/Devskiller/jfairy) or [zed](https://github.com/brimdata/zed).

My argument in this article is that mock data, when treated with respect, turns out to be a supremely useful, versatile and valuable tool.

I find four special uses for mock data:

1. Iteratively designing and documenting structures
2. Boosting unit test development
3. Manually testing a running application
4. Decoupling development teams

I will elaborate on each of these.

## Iteratively designing and documenting structures
Designing &amp; documenting

Let&#39;s begin with an example that should be reasonably familiar to most developers:

**Validating a username in a sign-up form.**

Suppose we want to capture and validate one of three kinds of username:

- an email address -or-
- a 9-character alpha string -or-
- a numeric employee number

We might begin by modelling the `username` field as a `string | number` data-type that covers all three kinds of username.

```typescript
type Username = string | number;
```

The above type expresses a part of our requirements but not all of them. Specifically, it doesn&#39;t express such facts as:

- the username might be an email
- the username might be a 9-character string
- the username might be a string longer than 9 characters, but that would be invalid

How might we express these requirements in code prior to writing a validation routine?

One way might be to write descriptive comments next to the field. Another way might be to bring in a hefty and cumbersome &quot;validation framework&quot; such as `zed` and try to twist and wrangle it to the shape of our specific requirements.

Or... we could simply create a few mock values:

```typescript
const UsernameMocks = ;
```

We have now enumerated the variants of the username field that we expect to deal with.

And we&#39;ve given them descriptive names. Naming them this way helps us to think clearly about the requirement and allows us to document the requirement in code.

Looking at these mock values, we might also begin to ask questions. For example: does a numeric username have a lower and upper bound? Safe assumptions might be `0` and `Number.MAX_SAFE_INTEGER`, but we might want to check with a domain expert.

The point is, by laying out all the expected values of this field together in one view, we give ourselves an opportunity to think more deeply about the range of possible cases and how we will deal with these cases. In this way, mock data becomes a kind of tool for concretising requirements.

We can now begin to think about how we might design and test the validation routine.

## Boosting unit test development
Unit tests

As we begin to write a username validation function, the `Username` type definition combined with `UsernameMocks` given above lead directly to four unit tests:

```typescript
describe(&quot;validateUsername&quot;, () =&gt; );

  it(&quot;validates text 9 chars long as true&quot;, () =&gt; );

  it(&quot;validates text more than 9 chars long as false&quot;, () =&gt; );

  it(&quot;validates number as true&quot;, () =&gt; );
});
```

This isn&#39;t a coincidence ‚Äì the whole point of mock data is to represent kinds of values we expect to deal with. Our unit tests need to do the same thing, to verify that the system under test behaves as we expect.

We can now begin to implement the requirement, writing each of the unit tests one by one, and writing enough code to make it pass.

Alternately / additionally, we could use this information about expected inputs to scaffold the function with comments:

```typescript
function validateUsername(username: Username): boolean 
```

This is just scratching the surface!

As we write the tests, we notice that we can reference the mock constants directly inside the test code, to avoid repeating ourselves.

For example:

```typescript
describe(&quot;validate username&quot;, () =&gt; );
});
```

Notice how clean and readable this test is!

The naming of the mock constant reveals the intent of the test beautifully. Rather than pollute our test code with concrete values, we can extract them to well-named constants and have the test code focus on the _relationships_ between the values.

The re-use can go even further. Suppose, in a different part of the code, we want to display a user profile which includes the user&#39;s username. We want to style the text differently depending on what kind of username the user has.

We might extract the username type and mocks to its own module, say, `username.ts`. Then we can re-use the mock usernames in our tests, like so:

```typescript
describe(&quot;user profile&quot;, () =&gt; );

  it(&quot;renders email username in sans-serif&quot;, () =&gt; );

  it(&quot;renders alpha username in sans-serif&quot;, () =&gt; );
});
```

In future we might need to maintain username code ‚Äì e.g. support additional kinds of username, remove support for a kind of username, fix a mistake in one of the mock values, etc.

It will be much easier to find the code that needs to change across the whole codebase if we use consistent mock constants than it will be if we use inconsistent mock literals.




    class UsernameMocks 
    UsernameMocks ..|&gt; Username

    class UsernameTests 
    UsernameTests ..&gt; Username
    UsernameTests ..&gt; UsernameMocks

    class UserProfileTests 
    UserProfileTests ..&gt; Username
    UserProfileTests ..&gt; UsernameMocks
`}

The end result is that any test code that deals with mock values will be:

- faster to write
- easier to read
- more maintainable

**Bonus:** And we&#39;re talking about more than just unit-tests here! Mock values can also be used to fill in live component demos (e.g. Storybook stories), dynamic documentation (e.g. Docusaurus pages) and snapshot tests (e.g. PhantomJS screenshots).

## Manually testing a running application
Manual testing

Imagine if our application was augmented with mock data in such a way that all of its features could be used while running purely off the mock data, without ever having to connect to any real data-source. (By &#39;data-source&#39;, I&#39;m referring to things like databases, APIs, etc.)

This capability would offer some unique advantages:

- We could simulate any behavior we desired in our application (by mocking states that could trigger that behavior)
- We could test how the application would respond to an unexpected state (by mocking that state)
- We could run and develop the application entirely on mocks while the external data-source was down, e.g. during an outage or planned maintenance
- We could develop new features in advance of the data-source supporting them (adding mock data as needed and only substituting real data as it becomes available)
- We could demo features of an application to stakeholders prior to having an external data-source to support that feature
- We could mock data to model changes to an external data-source, to clarify our own thinking and/or to communicate requirements to the data-source maintainers

Mocking all of an application&#39;s data-sources might seem like a daunting task. However, in my experience, it&#39;s easier than it might seem, especially if done in the early stages of a project.

But there are a few pre-requisites.

Firstly, the application needs an interchangeable data-source, so we can switch between real data and mock data. We need to write all our application code against that data-source abstraction, without concern for where the data actually comes from.


    B --&gt;|Yes| C[Use mock data]
    B --&gt;|No| D[Use real data]
`}

Secondly, we need to be able to switch the Application state between the real data-source and a mock data-source.

- Some Single Page Application (SPA) projects use a state container such as Redux. In that case, we might dispatch a `SetMockStateAction` which sets an `isMockMode` flag in the store. When this flag is `true`, all data-retrieval actions use mock data rather than making a real HTTP call.

- In other cases (typically a SPA or a micro-service), an HTTP API client sits between the Application state and the HTTP API and calls the HTTP API to fetch data. We could add a `setMockState` method here, which sets a private `isMockMode` field which, when `true`, uses mock data rather than making a real HTTP call. Or we could set up a Dependency Injection (DI) system, in which an abstract `APIClient` interface is implemented by `HttpAPIClient` and `MockAPIClient`, which can be substituted at runtime.

The &quot;switch&quot; could be activated by clicking a UI element, e.g. a small checkbox in the footer area of a web page, which is only visible to Admin users. The click handler toggles the Application state between real data-source vs. mocks.

![Screenshot of a mock data checkbox](/images/articles/mock-data/mock-data-checkbox.svg)

Thirdly, we need all of our application state to be mocked in a set of mock constants. This is easiest if the application state is modelled in some more abstract way, e.g. using classes, interfaces, types, etc. That way, we can easily construct our mock data as a realisation of those abstractions.



    class MockAccount 
    MockAccount ..|&gt; Account
    
    class Invoice 

    class MockInvoice 
    MockInvoice ..|&gt; Invoice
`}

It would take significant effort to completely mock the data-sources of a pre-existing application. But that effort could be broken down into smaller pieces and pursued incrementally, similar to adding unit tests.

Once our application is completely augmented with mock data, maintaining the mock data going forward would only add a small overhead (and might even boost development speed, as described in sections 1 and 2).

---

How about switching between _multiple_ mock states? One way would be via a collection of mock controls, presented to Admin users in the UI, allowing the mocks to be adjusted at whatever level of detail is needed. These could be presented in a pop-up modal or panel activated by clicking a Mock Settings button located somewhere out-of-the-way and only accessible to Admin users.

The following screenshot depicts mock controls for a hypothetical online banking application, allowing the user&#39;s home country, preferred currency and business / personal accounts to be adjusted.

![Screenshot of controls for more complex customization of mock data](/images/articles/mock-data/mock-data-controls.svg)

## Decoupling development teams

Decoupling teams

Once our entire Application is able to run off mocked data, not only can we _operate_ the application while a data-source is unavailable, but we can also _operate and develop_ application features prior to the data-source even supporting them.

**Mocking our data-sources decouples our team&#39;s development efforts from other teams.**

For example, imagine we are working in an _Invoicing_ team in an online banking system. We want to build a &quot;Foreign Accounts&quot; feature. Suppose this feature depends on data from an _Accounts_ team. Without mock data, the Invoicing team might have had to await until the Accounts team had built certain APIs, which it would then consume.

But with mock data, the Invoicing team no longer needs to wait for the Accounts team to support a Foreign Accounts feature, but rather, it can immediately begin developing the Foreign Accounts feature. We merely need to model Foreign Accounts interface in a way that makes sense for Invoicing purposes, and from those models, derive mocks.



We can develop and test all the Invoicing logic against those mocks. When the Accounts team does finally support Foreign Accounts we can connect our Invoices system to theirs. Any dissonance between their models and ours can be solved by adding a mapping layer, e.g. an `AccountsAPIDataSource` which implements `AccountsDataSource` methods by calling AccountsAPI methods.


    InvoicingApplication ..&gt; ForeignAccountsDataSource

    class ForeignAccountsDataSource 

    class ForeignAccountsAPIDataSource 

    ForeignAccountsAPIDataSource ..|&gt; ForeignAccountsDataSource
    ForeignAccountsAPIDataSource ..&gt; AccountsAPI
    
    class AccountsAPI 
`}

Notice how we are building our Foreign Accounts feature against a `ForeignAccountsDataSource`, which functions as a kind of contract between the `InvoicingApplication` (which we control) and the `AccountsAPI` (which the other team controls).

This contract helps us to think more clearly about what we need from the Accounts team - the inputs and outputs and behaviors. So we can communicate more clearly with the teams who we depend on about the data we depend on them for.

### Sharing mocked states with team members

With mocked data in place, we can share various configurations of our application with team members such as QA engineers / testers, product owners, usability and accessibility professionals and others.

One technique applicable to web applications (which I used on a recent real-life project) is to configure the mock states via queryString parameters in the URL. The URL can then be shared with anyone who needs to see the web application in the mocked state. Rather than having the team member go through complicated sequences of steps to simulate a given state, all they need to do is to open the link.

For example, if we want to simulate the state in which the user entered a credit card into a payment page, but the card has expired, we might share a URL like this: http://myapp.com/payment-details?mockCreditCardExpired=true. This URL could be used to test the error message that is displayed for expired cards.

## Other uses of mock data
Other uses

We&#39;re talking about more than just simulating expected (or unexpected) application states! We can also simulate large data-sets (e.g. to test scalability), error conditions (to test error handling logic), delays (to test performance under various network conditions) and... well... anything else it&#39;s possible and useful to simulate. The ability to simulate specific application states is kind of a super power.

## Summary

We&#39;ve looked at four interrelated benefits of treating mock data with respect and rigour, with examples / pseudo-code for each.

1. It helps us to clarify our software design by thinking about examples during modelling, before diving in to implementation.
2. It boosts our unit testing efforts by providing a ready-made set of test inputs and making test code more readable and maintainable.
3. It gives us the powerful capability to run our application independent of external data-sources and, as such, to simulate any application behavior we desire.
4. Finally, it decouples us from immediate dependency on other teams while clarifying the relationships between teams by encouraging us to model them as contracts.

These benefits come at a cost. Using mock data in this way requires application code to be structured in a certain way (isolation of Application state). And it takes significant effort to augment a pre-existing application with mock data, especially if that application has complex logic.

Given all of the above, mock data seems best suited to long-term software projects of moderate complexity, where the advantages of using mock data outweigh the costs.

## Related tools

- [Mock Service Worker](https://mswjs.io/), targeted at web/typescript projects, provides some infrastructure for mocking web endpoints. The application can make requests as usual, but MSW can handle the requests and provide mock responses as if the Backend was mocked. The requests will appear in the Network Tab of Developer Tools, just like a regular request.

## Further reading

Books that inspired me:

- [_Domain Modelling Made Functional_](https://pragprog.com/titles/swdddf/domain-modeling-made-functional/) ‚Ä¢ Scott WLASCHIN
- [_The Art of Unit Testing_](https://www.artofunittesting.com/) ‚Ä¢ Roy OSHEROV
</content>
  </entry>
  

  <entry>
    <title>Parallel loading in React</title>
    <link href="https://conwy.co/articles/parallel-loading-react" />
    <updated>2024-09-13T00:00:00.000Z</updated>
    <id>parallel-loading-react</id>
    <content xml:lang="en" type="html">Recently I had to build a React component that would merge the results of several independent requests into one set of data and render it as a single list.

The requests would look like this:

- GET to load initial list of items
- GET to load item 1 details
- GET to load item 2 details
- ... etc for each item in the list

I wanted the list to ultimately render like this:

- Item 1 + details
- Item 2 + details
- ... etc for each item in the list

The problem was: how to load a list of items, then load details of each item, making a separate request per item, then combine all of this information and render it to the user, all within one component.


  Note: This could be done more easily by splitting details into a separate component, rendered in a `map` loop, and having each instance of that component make its own request. However, for various reasons, this wasn&#39;t possible, and I had to do everything in one component.


## Synchronously combining results

The simplest way would be to await all the requests and then render them together at once.


    D --&gt;|Yes| C
    D --&gt;|No| E(Combine items list with details)
        E --&gt; F(Render items list with details)
        F --&gt; G[Done]
*/}

![Flowchart depicting synchronous loading of items](/images/articles/parallel-loading-react/sync-loading.svg)

Here is an implementation which uses `Promise.all`.

[Codepen Link](https://codepen.io/jonathanconway/pen/dyRQGam)

```jsx
function UsersAndStatuses(props) ));

    setUsers(usersWithStatus);
  }, []);

  return (
    
      
       
        
      ))}
    
  );
}
```

The problem with the above is:

**It could take a long time for all the requests to complete.**

We don&#39;t want to keep the user waiting for the whole list to load before they can see any results.

It would be better if we could

1. Load and quickly render the list of items without the details, then
2. Load and render the detail for each item as soon as each response is received


    F --&gt;|Yes| D
    F --&gt;|No| G[Finish]  
*/}

![Flowchart depicting parallel loading of items](/images/articles/parallel-loading-react/async-loading.svg)

## Asynchronously combining results

Implementing this improved solution raised a challenge:

**How to merge the details from all the requests together into one state variable without triggering a React refresh cycle?**

If the React refresh cycle triggered, it would have caused the state variable to contain incomplete data, as one partial value would override another.

It turns out the solution is rather simple: we just have to re-use the latest copy of our state variable each time we set it.

So instead of the typical `setState` call:

```js
setUsers();
```

We [pass a state setter](https://twitter.com/dan_abramov/status/816394376817635329) whose parameter (`currentUsers`) will always have the last updated value:

```js
setUsers((currentUsers) =&gt; ());
```

So... here&#39;s the parallel loading solution.

[Codepen Link](https://codepen.io/jonathanconway/pen/dyRQMQL)

```jsx
function UsersAndStatuses(props) );
  const users = React.useMemo(() =&gt; Object.values(usersById), [usersById]);

  React.useEffect(async () =&gt; ),
        
      )
    );

    const userIds = usersList.map((user) =&gt; user.id);

    userIds.forEach(async (userId) =&gt; ,
      }));
    });
  }, []);

  return (
    
      
       
        
      ))}
    
  );
}
```


</content>
  </entry>
  

  <entry>
    <title>Wireframing techniques</title>
    <link href="https://conwy.co/articles/interaction-wireframes" />
    <updated>2024-09-13T00:00:00.000Z</updated>
    <id>interaction-wireframes</id>
    <content xml:lang="en" type="html">It‚Äôs nearly 2021, so I thought I‚Äôd share a small achievement of 2020:

**A way of depicting interactions in UI wireframes!**

Background: I&#39;ve been doing a lot of complex UI work lately. In the process I&#39;ve been finding it useful to diagram out these interfaces. I like to do this both before and during the actual development work. It really helps to understand and reason about the UI. These are very interactive UIs - lots of clicking, dragging, dropping, etc. So the question arises: can the interactions also be expressed in visual form?

From experimenting with a number of projects and techniques, I&#39;ve settled on a consistent style, which I now use in all wireframes:

1. Elements ‚ñ° - Shapes and enclosed text
2. Markers ‚óã - Numbered circles (with a legend)
3. Interactions ‚Üí - Connecting arrow lines (with labels)

Let&#39;s do a quick dive into each.

## 1. Elements

The actual elements that make up the interface are marked out with shapes such as rectangles, rounded rectangles, circles, etc., similar to how they would appear in the final application.

Here&#39;s a convention to follow for shapes:

- Straight rectangles for panels, modals, headers, etc.
- Rounded rectangles for buttons, input boxes, checkboxes, etc.

And here&#39;s an example:

![Wireframe showing only interface elements drawn with simple shapes](/images/articles/interaction-wireframes/wireframes-elements.svg)

## 2. Markers

We might want to &quot;mark&quot; or &quot;tag&quot; a particular part of the UI with additional info.

For example, to indicate that a specific component should be used, when it isn&#39;t obvious just by looking at the diagram, we might want to mark that part of the wireframe with the name of that component.

For this purpose, we can drop in numbered circles, and along with a legend item for each indicating the name of the component.

Here&#39;s how it looks:

![Wireframe showing interface elements numbered with legends](/images/articles/interaction-wireframes/wireframes-markers.svg)

## 3. Interactions

Finally we get to the exciting part: _interactions_.

The user might generate events (click, drag, drop, keypress, etc) which cause the UI to change in some way (e.g. open a drop-down, move an element, hide a modal, etc).

You might have assumed that depicting interactivity would require the use of a prototyping tool, but actually it can be done in a static wireframe too.

We can simply add a connecting arrow lines made up of the following parts:

1. Source of the connector - the element from which the event originates
2. Label on the connector - name of the event
3. End of the connector - arrow-head pointing at a component (or group of components) which depict the state of the UI when the event is handled

For example, suppose we want to depict that when the user clicks a button, a modal box appears. We draw an arrow from the button to the modal, with ‚Äò(click)‚Äô in rounded brackets on the connector.

![Event connector depicting a button click event](/images/articles/interaction-wireframes/wireframes-interactions.svg)

Notice that we don&#39;t necessarily have to depict the whole UI in the &#39;after&#39; part of the wireframe. We only have to depict the part that changed - in this case, the modal. This habit of only depicting the change really speeds up the wireframing activity. We only have to depict _changes_ in our UI, not the whole UI in every possible state.

There might be some logic to these interactions. For example, if the user enters a correct username and password to log in, we display a success notification. But if they get the password wrong, we show an error message underneath the password field. This kind of logic can be depicted by augmenting our diagram with a flowchart shape.

## Putting it all together

Here&#39;s the whole UI - elements, numbered labels and event connectors (with logic).

![Wireframe showing interface elements, component labels and event connectors](/images/articles/interaction-wireframes/wireframes-alltogether.svg)

Notice how you can read and comprehend this quite quickly, just like we&#39;d read a paragraph out of a book. Wireframes can communicate a information that is better represented visually and spatially rather than in paragraphs of plain text.

Notice also that this wireframe can manipulated - split apart, combined with other elements, used to form a new wireframe. This can be an excellent tool for experimenting with alternative designs. It can also be great for communicating - you can easily slice of any part of the design, paste it into a Slack discussion, and gather some feedback from your colleagues.

## Conclusion

I&#39;ve found simple, low-fidelity wireframes that highlight **interactions** to be highly useful when developing highly interactive or logic-intense user interfaces.

- They help me to understand how the software will actually work.
- They give me a feeling of control over my work environment - I can change the design at any time and in any way.
- They can help me to think clearly and form a good mental model of the requirements.
- They provide a visual aid for communicating requirements to team members and getting their feedback.
- They serve as a guide and reference point while I&#39;m actually writing the code.

## Further reading

Books that inspired me:

- [_Designing for the Digital Age_](https://www.wiley.com/en-us/Designing+for+the+Digital+Age%3A+How+to+Create+Human+Centered+Products+and+Services-p-9780470229101) ‚Ä¢ Kim GOODWIN
- [_Macintosh Human Interface Guidelines_](https://developer.apple.com/design/human-interface-guidelines/) ‚Ä¢ Apple Computer
</content>
  </entry>
  

  <entry>
    <title>Towards zero bugs</title>
    <link href="https://conwy.co/articles/towards-zero-bugs" />
    <updated>2024-09-13T00:00:00.000Z</updated>
    <id>towards-zero-bugs</id>
    <content xml:lang="en" type="html">Software with zero bugs may seem like an ambitious goal. Over time, defects in software have increased and have become so normalised that some developers and users even expect them.

But while it&#39;s difficult to get to zero bugs, I think it&#39;s worth trying for. We shouldn&#39;t concede defeat and assume ahead-of-time that our products will be defective. Rather, we should do everything in our power to avoid inadvertently creating bugs in our software, where they could be avoided. The closer we get to zero critters, the better!

Over time, I have been building up a mental checklist of things to look out for, both in the code I write and in the running application that it generates, to identify potential bugs. I now run through this checklist whenever I am about to complete work on a change or a new feature. I have also been working on building a mindset that encourages discipline, rigour and attention to detail.

By running these checks and building this mindset, I aim to identify and fix bugs early, rather than having them show up in a testing environment, or worse still, in front of an end-user.

I would love to share this with other developers. Please have a read and let me know your thoughts in the comments!

## The checklist

Without further ado, here&#39;s my list:

**Typos, accidental keystrokes, debugging statements.** Every time you&#39;re about to commit, hold back for a moment and review the diff of changes going in. Make sure you&#39;re only committing what you fully intend to commit. Check for typos, accidental keystrokes, inadvertent capitalisation, etc. A compiler or linter can usually pick these up, but there are often cases that are missed, so it&#39;s still worth taking a few seconds to run your eyes over the diff. Also check for development-only code, such as logging or debugging statements, which pass compilation but shouldn&#39;t be checked in.

**Subtle logic errors.** Look for all those mistakes that _look_ like reasonable code, to both the first glance and the compiler, but are actually the wrong way round or otherwise incorrect.
For example:

- False-positives. For example: `if (!hidden)  else `. Observe that `!hidden` is actually equivalent to being visible. So this code would actually execute `show()` when already visible and `hide()` when invisible! To correct this, we would want to remove the `!` and have something like this: `if (hidden)  else `. It&#39;s important to keep an eye out for these kinds of subtle logic errors.
- Expressions being coerced to incorrect boolean values. For example, in Javascript, an `indexOf(x)` call without being compared to anything, when it should be compared it to a numeric value. A correct (and clearer) way to achieve this intent might be to call `includes(x)`, which _does_ return a boolean.
- Off-by-one errors. For example: `for (let i = 0; i &lt;= 10; i++) `. This loop runs 11 iterations, where it was probably expected to run 10. It would be clearer to rewrite it as: `for (let i = 0; i &lt; 10; i++) `.
- Filtering operations. You may perform a filtering function, but accidentally extract items from a list and return only those items, when your intent was to return the full list _including_ those items. Or your code might return everything except certain items, when the intent was to return nothing at all if those items exist. There are many other variations on this. In summary, carefully review complex filtering operations.

**Edge cases.** To find these, try to break your app.

- Click a lot of different parts of the UI in very quick succession.
- Test long sequences of actions and make sure the result at the end is exactly as expected. For example, test undo/redo thoroughly by performing an action, then undoing it, then redoing it, many times, then verifying the end result.
- Input values in unexpectedly large quantities, in an unexpected format or null/empty values.
- Test with correctly formatted but illogical values (e.g. a date that is the 32nd of the month).
- Add a larger than normal number of items to a list.
- Run multiple instances of the application at once and verify that it still works properly.

Basically do everything you can to break your application and ensure that it recovers gracefully in all circumstances. If you have a large number of possible combinations of inputs to test, unit tests can definitely be your friend!

**Values vs. references.** Do you expect a value to be set in one place and updated in many others? Or do you want to hold independent copies of that value in multiple places? Review your usages of references vs values and make sure they&#39;re correct for your use case.

**Memory leaks.** These can dramatically slow down an application and even cause it to crash, due to incorrect and unconstrained allocation of memory. These can manifest themselves in a variety of ways, depending on the language and environment you&#39;re developing for.
For example:

- In C# or Java, it may be an unmanaged resource that&#39;s not being cleaned up.
- In multithreaded applications, dead threads.
- In Javascript, Maps that reference DOM nodes that no longer exist.
- In RXJS, subscriptions to observables that you forgot to unsubscribe.

In addition to manually checking the code, practically every environment also has its own set of tools for diagnosing memory leaks. For example, for .NET, there is a memory profiler and for Javascript, Developer Tools in most browsers have a Memory tab or similar.

**Code executing too often.** Do you perform unnecessary operations within a for loop, a game loop, a template, a rendering cycle, or any other part of the code base that gets executed many times in succession? This could cause a slowdown to your app, which if it gets too bad, could be considered buggy behaviour. Code that might not need to run includes code that generates the same result on every iteration (in which case, some form of caching is your friend) or code that&#39;s only needed in certain states (where a simple `if` statement around that state could skip the code when it&#39;s not needed).

**Same same but different.** Be extra careful in situations where you have two things that look and behave very similarly, but are qualitatively different. An example of such a situation, which I encountered recently, was in building two tree views which depicted essentially the same data, but with subtly different visual markers on each. These visual markers highlighted opposite aspects of the same data. But, by mistake, I also coded one of the trees so that it reversed the order of its elements! This bug should have been obvious, but it escaped my notice. I was so focussed on getting the markers right (the difference) that I forgot to ensure that the ordering was right (the sameness). In retrospect, if I had pulled back and double-checked that the end-result had the _right_ difference and not the _wrong_ difference, I could have caught this early and fixed it.

**Null-checks.** Whenever two values are being compared, have you null-checked and undefined-checked both sides of the comparison if needed, and handle what to do if either/both are null? Add checks as needed. (Some languages offer conveniences / syntactical sugar for this. E.g. Javascript has the [optional-chaining operator](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Optional_chaining): `?.`.)

**Async data dependencies.** Does your app depend on multiple sets of data, which may load at different times? What happens when not all of the data has loaded? Does the application crash and burn? Or does it handle the situation gracefully, perhaps waiting until all the data has loaded, and showing a &#39;loading&#39; indicator in the meantime? You might simulate this state by temporarily adding a lag to one of your data sources, using your language&#39;s &#39;delay&#39; mechanism. For example, calling Javascript&#39;s `setTimeout` method, RX&#39;s [Delay operator](http://reactivex.io/documentation/operators/delay.html) or .NET&#39;s `Thread.Sleep()`. Of course, take care to revert any testing code prior to check-in!

**Browser/OS upgrades.** Depending on the environment you&#39;re developing for, be aware of the potential for breaking changes to that environment, when a new version comes out. Upgrade whenever a new version ships and test your application in the new version, looking for bugs. I experienced the importance of this recently, with the [changes to Flexbox in Chrome 72](https://bugs.chromium.org/p/chromium/issues/detail?id=927066), which necessitated several CSS changes.

**Devices, screen sizes and zoom factors.** Test your app with multiple devices if needed ‚Äì mobile, tablet and/or desktop. You may also need to check with multiple browsers on those devices as well as multiple versions and form-factors of the devices. Also, try increasing/decreasing the zoom level and ensure that the layouts, sizing, etc, are still proportional.

**Accessibility.** Bugginess or even absence of accessibility features is a major problem in the software application landscape. If your app will be used by a broad segment of the population, you probably should be ensuring that it is accessible. Ideally accessibility is &quot;baked-in&quot; from the beginning, but this doesn&#39;t nullify the need to regularly and rigorously test that accessibility features work. In my own accessibility auditing, I focus on three main areas: A) keyboard-only operation, B) non-visual operation, C) adherence to WCAG. A basic test of these three areas can be performed on any web page, by A) pushing the mouse away and attempting to use the application keyboard-free, B) looking away from the screen and attempting to use the application by means of only a screen-reader, C) running the [Wave](https://wave.webaim.org/extension/) automated testing tool and reviewing its output. Similar tests can be run on non-Web/native applications. I plan to write an entire article dedicated to this topic, as it is a large one. In the meantime, you can check out some excellent resources, such as [WAI](https://www.w3.org/WAI)&#39;s [Easy Checks](https://www.w3.org/WAI/test-evaluate/preliminary/) page.

**Date and time handling and formatting.** Be extra careful to test code that does anything with dates or times. If the code is performing some kind of calculation on a date/time value, try to test it with a variety of inputs and ensure that it always produces a correct result date/time. Also, test that it works in a different time-zone. To do this locally, you can temporarily change your system time-zone, re-load your application and re-test the date/time feature.

**Numeric values, such as currency.** As with dates/times, thoroughly test any aspect of your application that operates on numbers, and especially locale-specific numbers such as currency values. Also check if you might receive a numeric value as a string and need to convert it to an appropriate numeric type before using it.

**Load testing.** Does the system break down when large number of items are passed through it? Substitute a fake data-source with thousands or even millions of records and see if the application can handle that load.

**Requirements vs solution.** Double-check the original requirements and see if you actually addressed them. There might have been a subtle indication in the language that you overlooked or some ambiguities that you didn&#39;t yet clear up. If you need to go back to the business to clarify these issues, do this as soon and early as possible, so that you have a better chance of fixing any bugs in the code before releasing it.

**Hit refresh.** Sometimes, for reasons that I don&#39;t entirely understand (and perhaps don&#39;t wish to) a running application will get out-of-sync with the code that generated it. Yes, this can happen even when automatic compilation tools are in use. In the case of web apps, caching of assets can play a role. For native apps, processes may remain open. I have sometimes spent half an hour or more trying to figure out why something wasn&#39;t working or why I couldn&#39;t reproduce a bug, only to find that the version I was using was stale. Long story short: when in doubt, hit restart and refresh.

**Multiple environments.** Most organisations have multiple environments into which software is deployed in a staged manner. There&#39;s the local developer machine, then a Development server, then Staging and/or QA, then Production/Release/Live. It&#39;s a good idea to run some tests on your application in every environment. This is especially important if your feature or change depends on environment-specific factors, such as configuration values, database schemas, data and other systems, services or resources. Anything might go wrong in a new environment, from a typo in a configuration value to a missing authorisation on a resource. You don&#39;t have to test everything in every environment, but it&#39;s probably a good idea to at least test the happy path.

**Find similar bugs and fix them (and generalise the fix!).** This came up recently, where a colleague discovered a bug in which the wrong property was being used to retrieve the error message from an HTTP response. Rather than merely fixing it for that one response, I tested all places in the codebase where an error message was being retrieved from an HTTP response and fixed them all where necessary. I then went a step further and generalised the fix, by extracting HTTP error handling to a common function. So not only were additional bugs eliminated, but similar bugs in the future were prevented, by improving the overall framework.

**Errors of addition.** When adding new code, be careful that it doesn&#39;t cause an error. For example, adding a field to a class, adding a value to an enum, etc might cause unexpected behaviour. This is especially important if you have code somewhere that dynamically reads the structure you&#39;re modifying, e.g. code that loops over the fields in a class using reflection. (Such &quot;dynamic access&quot; is usually not best practice, but unfortunately some code-bases use it, so we might need to check the code-base we&#39;re working on.)

**Errors of ommission.** When adding new code, be careful that we didn&#39;t *forget* to include something, which might cause an error. Say we create a new subtype of an inheritable class, we might need to include some field or value. This might not necessarily be indicated by the compiler if, e.g., our code-base has some dynamic code that loops over the fields in all subtypes of the class and expects certain fields to exist.

**Consuming a data source in a context where it is not available.** When we call a method or function from a component, we might verify that our code works by using that component and seeing that it works correctly. But will that call work in every possible context in which the component is used? What if there is a different way to access the same component, in which that call breaks? This could be very subtle and easy to miss, if we are not aware of the different contexts in which our component is used. For example, this happened to me once when working on a popup modal in React. The modal consumed a hook which depended on certain data being in the browser URL. But I was not aware that the modal could be accessed from a different page with a different URL which did not have that data. The different URL broke the hook and thus my modal component.

**Re-testing after merge.** After completing a change and pushing, you might need to resolve a merge conflict or rebase your change. Be careful to re-test your work following the merge! Even a successfully automated merge might still result in a subtle logic error that you missed. The same applies to any changes you make in response to pull-request comments, build errors, etc.

**Remote API calls.** Ensure all remote API calls your code depends on are fully working. E.g. HTTP requests, web-sockets connections, etc.

## The mindset

This checklist may seem daunting, especially when working under time constraints. However, you don&#39;t have to action all of these items for every change you make. I typically give this list a quick scan and pick out only the items that are relevant to the change I&#39;m making. For example, a change to the logic for calculating a numeric value probably doesn&#39;t necessitate checking &#39;Devices, screen-sizes and zoom factors&#39;. Likewise, for a change to the layout of a dialog box, I can probably skip &#39;Async data dependencies&#39;.

The &quot;old&quot; mindset (that I have sometimes seen in the industry) is:

- I assume my code has no bugs by default.
- Good developers never write buggy code, so I shouldn&#39;t bother too much checking my code for bugs, otherwise I might discover that I&#39;m a terrible developer!
- There&#39;s never enough time to check for bugs, so I have no choice but to ship buggy code.
- My code will naturally get more and more reliable as I gain experience.
- Testing and bug-fixing is boring, tedious and not fun.
- There&#39;s no reward to being thorough about testing for and fixing bugs.
- Software development is unimportant, menial &quot;grunt work&quot;, so it doesn&#39;t matter if we get it wrong.

The &quot;new&quot; mindset that I aim to spread, which I think is more productive, is:

- My code is buggy unless proven otherwise.
- Part of being a good developer is having the discipline and patience to go through code that I wrote, which looks fine - even spectacular - and find and fix all the bugs that I know are probably lurking within it.
- There&#39;s almost always a little extra time to put in some honest effort to finding and fixing bugs.
- Putting in a regular, consistent effort to write reliable code will make my code more reliable.
- Testing and bug-fixing can be made fun, with a positive mindset and a little &#39;gamification&#39;. I can enjoy the endorphin-rush of fixing a bug and knowing that I left the code better than I found it.
- The reward to testing for and fixing bugs is building the mental muscles (discipline, rigour, attention to detail, etc) that will result in more reliable software. Those muscles will move me forward in all aspects of problem-solving, not only bug-fixing. Also, I can build a reputation as someone who builds reliable software, which will probably be good for my career.
- Software development is a profession and a craft, and we should take pride in our work.

## Let a thousand checklists bloom!

Do you keep a checklist like this, either in written or mental form? Are there any other items you would add to such a checklist? And do you have anything to add about the mindset needed to write reliable, bug-free code?

Feel free to comment about your checklists and experiences or link to them in the comments. It would be great to share any ideas that we developers can use, in order to get closer to writing bug-free code.

Thanks for reading!

## Further reading

Boooks that inspired me:

- [_The Checklist Manifesto_](https://atulgawande.com/book/the-checklist-manifesto/) ‚Ä¢ Atul GAWANDE
- [_Code Complete_](https://archive.org/details/code-complete-2nd-edition/page/428/mode/2up) ‚Ä¢ Steve MCCONNELL
- [_The Pragmatic Programmer_](https://pragprog.com/titles/tpp20/the-pragmatic-programmer-20th-anniversary-edition/) ‚Ä¢ Andrew HUNT, David THOMAS
- [_Clean Code_](https://www.amazon.com/Clean-Code-Handbook-Software-Craftsmanship/dp/0132350882) ‚Ä¢ Bob MARTIN
</content>
  </entry>
  
  </feed>